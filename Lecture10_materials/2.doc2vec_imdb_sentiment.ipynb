{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу определения тональности рецензии фильма из датасета IMDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from os.path import join\n",
    "DATA_DIR = 'data/imdb_sentiment'\n",
    "\n",
    "import numpy\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Датасет состоит из следующих документов\n",
    "The result is to have five documents:\n",
    "\n",
    "- `test-neg.txt`: тестовый набор из 12500 негативных рецензий на фильмы\n",
    "- `test-pos.txt`: тестовый набор из 12500 позитивных рецензий на фильмы\n",
    "- `train-neg.txt`: обучающий набор данных из 12500 негативных рецензий на фильмы\n",
    "- `train-pos.txt`: обучающий набор данных из 12500 позитивных рецензий на фильмы\n",
    "- `train-unsup.txt`: неразмеченный сет из 50000 рецензий\n",
    "\n",
    "Для всех текстов уже проведена минимальная предобработка (приведение к нижнему регистру, удаление пунктуации,...)\n",
    "Каждая рецензия идет с новой строки файла "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скармливаем тексты в Doc2Vec\n",
    "\n",
    "Определим класс, который преобразует данные к формату, который ожидает док2век. Помним, что при обучении мы учим вектора для текстов, передавая внутрь некий условный айдишник документа в базе. \n",
    "\n",
    "Это то, чем док2вес отличается от ворд2века. Он интерпретирует айдишник документа как \"специальное слово\" и учит для него свой вектор, который и является представлением документа. \n",
    "\n",
    "Таким образом, док2век ждет данные в формате\n",
    "```python\n",
    "[['word1', 'word2', 'word3', 'lastword'], ['label1']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        doc_ids = {}\n",
    "        \n",
    "        # тут нужно убедиться, что id для всех документов уникальны (мы ведь его будем подавать внутрь алгоритма)\n",
    "        for key, value in sources.items():\n",
    "            if value not in doc_ids:\n",
    "                doc_ids[value] = key\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "        \n",
    "    def to_array(self):  # так как у нас есть несколько файлов с выборками, надо прочитать их все (даже неразмеченную)\n",
    "        self.sentences = []\n",
    "        self.textbook = {}\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(join(DATA_DIR, source)) as fin:  # читаем каждый файл с рецензиями\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    doc_label = prefix + '_%s' % item_no\n",
    "                    self.textbook[doc_label] = line\n",
    "                    sentence_words = utils.to_unicode(line).split()\n",
    "                    tagged_line = TaggedDocument(sentence_words, [doc_label])\n",
    "                    self.sentences.append(tagged_line)\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):  # рандомно перемешиваем тексты (это значительно влияет на качество)\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "# sources - отображение \"файл с текстом\" -> \"префикс для метки\"\n",
    "sentences = LabeledLineSentence(sources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Строим словарь\n",
    "\n",
    "Перед обучением док2век строит словарь по корпусу тектов, считает частоты слов, при необходимости удаляет редкие слова\n",
    "\n",
    "\n",
    "- `min_count`: выбрасываем из рассмотрения все слова, которые появляются меньше чем min_count раз в выборке. Здесь нужно поставить =1, потому что айдишники документов у нас уникальны (встречаются 1 раз) и тоже интерпретируются как слова\n",
    "- `window`: размер окна контекста, все как обычно\n",
    "- `vector_size`: размерность получаемого вектотра. 100 - обычно ок. можете попробовать 300 или 500, но запаситесь кофе. ждать придется подольше\n",
    "- `sample`: \"тот самый порог\" для downsampling'а слишком частотных слов (как в ворд2веке)\n",
    "- `workers`: количество воркеров, зависит от \"мощей\" вашей тачки (ставьте равным количеству ядер). \n",
    "- `epochs`: сколько раз хотим пройтись по всей выборке \n",
    "- `dm`: режим обучения doc2vec. Если стоит dm=1 (по умолчанию) - будет учиться PV-DM (distributed memory). Если dm=1 - PV-DBOW\n",
    "- `dm_mean`: параметр для PV-DM (при dm=1). Определяет то, как будут аггрегироваться информация для вектора текста и векторов слов при обучении. dm_mean=0 - суммирование вектора текста и всех векторов слов, dm_mean=1 - усреднение всех векторов\n",
    "- `dm_concat`: параметр для PV-DM (при dm=1). Конкатенация вектора текста и векторов слов (вместо уреднения/суммирования)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определим PV-DM модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 s, sys: 1.13 s, total: 18.3 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Doc2Vec(dm=1, epochs=10, min_count=1, window=10, vector_size=100, sample=1e-4, negative=5, workers=4)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение Doc2Vec\n",
    "\n",
    "Запустим обучение модели. Обучается лучше, если после каждой эпохи перемешивать выборку. Поэтому в классе LabeledLineSentence есть метод sentences_perm, который выполняет ровно эту задачу.\n",
    "\n",
    "Давайте запустим на 10 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 1s, sys: 21.8 s, total: 7min 23s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(sentences.sentences_perm(), epochs=model.epochs, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним модель для подгрузки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(join(DATA_DIR, 'imdb_10epochs.d2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(join(DATA_DIR, 'imdb_10epochs.d2v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим, чему научилась модель\n",
    "\n",
    "Давайте проверим, как модель научилась строить вектора для слов в процессе обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.8218690156936646),\n",
       " ('awful', 0.8145092129707336),\n",
       " ('horrible', 0.7970844507217407),\n",
       " ('good', 0.7474997043609619),\n",
       " ('lousy', 0.7229648232460022),\n",
       " ('stupid', 0.7059834599494934),\n",
       " ('poor', 0.678953230381012),\n",
       " ('crappy', 0.6580066680908203),\n",
       " ('lame', 0.6575591564178467),\n",
       " ('really', 0.6417423486709595),\n",
       " ('cheesy', 0.6368817090988159),\n",
       " ('just', 0.632099449634552),\n",
       " ('dumb', 0.6202185153961182),\n",
       " ('decent', 0.6072367429733276),\n",
       " ('ridiculous', 0.6067180037498474),\n",
       " ('movie', 0.6021982431411743),\n",
       " ('laughable', 0.5955305099487305),\n",
       " ('even', 0.5954839587211609),\n",
       " ('atrocious', 0.5939431190490723),\n",
       " ('ok', 0.5769880414009094)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('bad', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, что вполне себе адекватно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем для каждого документа посмотреть его обученный вектор, обратившись по айдишнику данного длокумента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05684923, -0.07725863,  0.13775468,  0.12887442,  0.00908506,\n",
       "        0.01480276,  0.02513479,  0.18365568, -0.05325323, -0.10734014,\n",
       "       -0.16174875,  0.10802115, -0.02439884,  0.06430485,  0.07098597,\n",
       "        0.09168662,  0.06569406, -0.13009293,  0.1423154 , -0.03993802,\n",
       "       -0.15709767, -0.00654513, -0.09003363,  0.05924307, -0.241081  ,\n",
       "       -0.12206589, -0.03812149, -0.04539146,  0.03792131, -0.22440206,\n",
       "       -0.1265503 , -0.06980037, -0.09843055, -0.03765225,  0.07679053,\n",
       "       -0.14751945, -0.00324497, -0.14627582, -0.28507626,  0.15701021,\n",
       "        0.0921864 ,  0.16014545,  0.0012446 ,  0.3154778 , -0.05361024,\n",
       "        0.05003441, -0.00273197, -0.30428532, -0.07969783, -0.0585442 ,\n",
       "       -0.09689523,  0.01964809, -0.1155773 ,  0.0268969 , -0.00834335,\n",
       "        0.05198096,  0.22581883,  0.02129975, -0.08291249,  0.05874895,\n",
       "       -0.12564994,  0.01517223, -0.1587225 ,  0.04311007, -0.13332868,\n",
       "       -0.05965387,  0.1506662 ,  0.11073697,  0.02142926,  0.09082322,\n",
       "       -0.07947854, -0.03641802, -0.00926819, -0.22670108, -0.07580371,\n",
       "       -0.13248035, -0.06197056, -0.02449069, -0.1851598 , -0.18841861,\n",
       "        0.14259437, -0.13760553,  0.00324686, -0.08725756,  0.00487843,\n",
       "       -0.17902568, -0.10551415,  0.20021755,  0.21263456,  0.12795421,\n",
       "       -0.08984204,  0.02376266,  0.04244632, -0.14045411,  0.17936207,\n",
       "        0.05632553,  0.03138905,  0.03396908, -0.12735955,  0.17649606],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['TRAIN_NEG_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query:\n",
      "b'an absolute classic the direction is flawless the acting is just superb words fall short for this great work the most definitive movie on mumbai police this movie has stood the test of timesom puri gives a stellar performance smita patil no less all the actors have done their best and the movie races on thrilling you at every moment this movie shakes your whole being badly and forces you to rethink about many issues that confront our societythis is the story of a cop om puri who starts out in his career as a honest man but ultimately degenerates into a killer the first attempt in bollywood to get behind the scenes and expose the depressing truth about mumbai cops kudos to nihalani after this movie a slew of bollywood movies got released that exposed the criminalpoliticianpolice nexus thus this movie was truly a trend setter this trend dominated the hindi movie scene for more than a decade this movie was a moderate box office hit a mustsee for discerning movie fans\\n'\n",
      "-----------\n",
      "b'a difficult film to assessthe cinematography is stunningone of the most beautifully shot and constructed urban thrillers ive seen from any country visually it surpasses most american productions in its lushness and moments of brutality in terms of atmosphere its unforgettable the character in the rain in the shadows of alleyways the iridescent shimmering interiors shot in some beautifully seedy spanish settingsa wonderful starturn by victoria abril also carries the film her character who cruises for sex in public places is in turns pathetic and fascinating and drives the film forward even when the plotlines lagwithout blowing the revelation of the film this also proves to be one of the most virulently aids and homophobic stories committed to film the films positing of a gay character as a crux of the mystery and the final moments of the story leans on a highly questionable exploitation of audience prejudices and anxieties taken at face value its an entertaining wellpaced thriller behind the surfaces though its reactionary trash which preys on hatred and misunderstanding of sexual minoritiesthe audience i saw this film with applauded at the end for the bravura filmmaking and good acting i left with a bad taste in my mouth and ashamed that i had enjoyed the majority of the film until the last few minutes depending on where youre coming from and how far youre willing to scrutinize the context of this film it will seem like a good wellmade movie or typically heterocentric bashing of gay people so common in movies from any country or perhaps both\\n'\n",
      "-----------\n",
      "b'i was amused by american reviews of the were taken by surprise how dare those russians to make a film that depicts americans as evil gangsters how dare danila to shoot americans this is not politically correct it was clearly assumed that russians are always the bad guys whose only role in any film is cannon fodder to be shot by american rambonow arrives uncovering the horrifying truth the bad guys are americans and it is ok to shoot themafter years of hollywood films depicting russians as bad asses no single hollywood film showing russians at least from a neutral point of view the russian cinema finally strikes backthis movie delivered a longawaited entertainment for russian viewersit also proved complete lack of sense of humor of american viewers\\n'\n",
      "-----------\n",
      "b'this is an extraordinary film as a courtroom drama its compelling as an indictment on the american justice system its frightening for brenton butler the consequences of this system could be devastating this film highlights the fundamental flaws of the legal process that its not about discovering guilt or innocence but rather is about who presents better in court in truth the implications of this case reach beyond the possibility of an innocent man being found guilty or a guilty man being free every citizen has a right to justice whether a perpetrator or a victim but do they get it the film is well paced understated and one of the best courtroom documentaries ive seen\\n'\n",
      "-----------\n",
      "b'police story is arguably one of the best works by the master of action himselfcompared to other action filmspolice story makes schwarzenegger and stallone look like beginnersthe stunt scenes are well cheorgraphed and the action scenes are superbif new line cinema has any sensethey would release this in theaters\\n'\n",
      "-----------\n",
      "b'directed by govind nihalani this is definite cop film of indian cinema may be the first one which portrayed the stark reality of corruption in the police force politics with no holds barred how it effects on a young cop a man forced to join a career of a cop by his cop father agreed that we grew up watching lot of good copbad cop hindi films but this is different todays generation which grown up watching dark realistic films like satya company may be consider it inferior product in comparison but look at the time of its making the film was made absolutely off beat tone in the time when people didnt pay much attention to such kind of cinema yet it becomes a most sought after cop film in class mass audience when it released for om puri its first breakthrough in mainstream hindi cinema he delivered a class performance as inspector velankar its more than cop character he internalized a lot which is something original in acting watch his scenes with his father whom he hates smita whom he loves smita patil maintained the dignity of her character to the expected level my god what a natural expressions she carried shafi inamdar was truly a discovery for me hes a brilliant character actor if given a chance here in some of the scenes he outsmarted even om the movie is also a debut of a promising villain on indian screen sadashiv amrapurkar as rama shetty its another story that he didnt get such a meaty role almost forgotten today as one of the loud villain of dharmendras b grade action films watch the scene where om time becomes a rebel for his father played by amrish puri next both are sharing wine together how inner truth started revealing for both the character with confronting feelings of love hate for each other two faces of indian police force masculinity impotency and in between lies half truth ardh satya kudos to nihalanis touch the film won national awards as best hindi feature film best actor om puri filmfare awards in best film best director best supporting actor categories recommended to all who are interested in nostalgia of serious hindi filmsratings\\n'\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "query_id = 'TRAIN_POS_120'\n",
    "print('Original query:')\n",
    "print(sentences.textbook[query_id])    \n",
    "print('-----------')\n",
    "\n",
    "similar_doc = model.docvecs.most_similar(query_id, topn=5)\n",
    "\n",
    "for tag, prob in similar_doc:\n",
    "    print(sentences.textbook[tag])\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценим, насколько хорошо получились вектора для документов на внешней задаче - предсказания тональности рецензии\n",
    "\n",
    "Используем полученные вектора текстов, чтобы построить классификатор оценки тональности.\n",
    "\n",
    "Создадим такой датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((25000, 100))  # 12500 позитивных и 12500 негативных\n",
    "train_labels = numpy.zeros(25000)  # метка сентимента\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model[prefix_train_pos]  # model[prefix_train_pos] - вектор для документа с id=prefix_train_pos\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training array looks like this: rows and rows of vectors representing each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05842066  0.08639163  0.32506874 ... -0.02422987 -0.31255844\n",
      "  -0.04418823]\n",
      " [-0.32492828  0.36115882  0.49848297 ... -0.04447221 -0.08652838\n",
      "  -0.32622376]\n",
      " [ 0.04599966  0.05331992  0.09771408 ... -0.07748941 -0.14217965\n",
      "   0.09537445]\n",
      " ...\n",
      " [-0.2491194   0.01202065  0.11303478 ...  0.02232761  0.11644159\n",
      "   0.00602041]\n",
      " [ 0.19666162  0.08343677  0.32887426 ...  0.03588881  0.14314343\n",
      "   0.33295429]\n",
      " [ 0.02957906  0.08606755  0.23530437 ... -0.03992742 -0.07587323\n",
      "   0.12089675]]\n"
     ]
    }
   ],
   "source": [
    "print (train_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are simply category labels for the sentence vectors -- 1 representing positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print (train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим такой же датасет для тестовых данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((25000, 100))\n",
    "test_labels = numpy.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем обучить логистическую регрессию поверх векторов документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver = 'lbfgs')\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим, какого качества удалось достичь на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = classifier.predict(test_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.842\n",
      "Recall: 0.8383782061494345\n"
     ]
    }
   ],
   "source": [
    "prec = precision_score(pred_test, test_labels)\n",
    "recall = recall_score(pred_test, test_labels)\n",
    "\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вполне себе ничего. попробуйте улучшить! =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
