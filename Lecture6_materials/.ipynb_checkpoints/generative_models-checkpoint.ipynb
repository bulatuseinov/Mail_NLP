{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  _Лекция 1: Языковые модели, сравнение строк_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части ноутбука вы посмотрите, как рвботают простейшие генеративные языковые модели, основанные на триграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_alphabet = re.compile(u'[а-яА-Я0-9\\-]+|[.,:;?!]+') # clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вспомогательные ф-ии, возвращающие генераторы с линиями  и токенами загруженного корпуса текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lines(corpus):\n",
    "    data = open(corpus)\n",
    "    for line in data:\n",
    "        yield line.lower()\n",
    "\n",
    "def gen_tokens(lines):\n",
    "    for line in lines:\n",
    "        line = re.sub('\\.{2,3}', '\\.', line)\n",
    "        for token in r_alphabet.findall(line):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация словесных триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trigrams(tokens):\n",
    "    t0, t1 = '$', '$'\n",
    "    for t2 in tokens:\n",
    "        yield t0, t1, t2\n",
    "        if t2 in '.!?': # если это конец предложения\n",
    "            yield t1, t2, '$'\n",
    "            yield t2, '$','$'\n",
    "            t0, t1 = '$', '$' # начало следующего предложения\n",
    "        else:\n",
    "            t0, t1 = t1, t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализуем простейшую триграммную генеративную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_trigram_model(corpus):\n",
    "    lines = gen_lines(corpus)\n",
    "    tokens = gen_tokens(lines)\n",
    "    trigrams = gen_trigrams(tokens)\n",
    "    \n",
    "    (bg, tg) = (defaultdict(lambda: 0.0) for i in range(2))\n",
    "\n",
    "    for t0, t1, t2 in trigrams:\n",
    "        bg[t0, t1] += 1\n",
    "        tg[t0, t1, t2] += 1\n",
    "\n",
    "    model = {}  # объект model - простейший словарь, хранящий вероятности следующего слова для каждой биграммы\n",
    "    ## Пример:\n",
    "    # model_tg_pure['онегин', 'был']\n",
    "    # Out: [(',', 0.5), ('готов', 0.5)]\n",
    "    \n",
    "    for (t0, t1, t2), tg_cnt in tg.items():\n",
    "        tg_prob = tg_cnt/bg[t0, t1]   #обратите внимание на оценку вероятности триграммы\n",
    "        if model.get((t0, t1)):\n",
    "            model[t0, t1].append((t2, tg_prob))  # (след. слово, вероятность)\n",
    "        else:\n",
    "            model[t0, t1] = [(t2, tg_prob)]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функции, возвращающие следующий токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_start_token(seq):\n",
    "    \"\"\"Возвращает рандомный стартовый токен, сэплируемый в соответствии с вероятностью встречаемости в тексте\"\"\"\n",
    "    seq_ = [x for x in seq if x[0].isalpha()]  # не будем стартовать с пунктуации\n",
    "    tok_seq, probs = [x[0] for x in seq_], [x[1] for x in seq_]\n",
    "    probs = [x/sum(probs) for x in probs]  # для np.random.choice сумма вероятностей должна суммироваться к 1 \n",
    "                                           #(нарушается, если выбросили пунктуационные токенв)\n",
    "    return np.random.choice(tok_seq, p=probs)\n",
    "\n",
    "def rand_prob_token(seq):\n",
    "    \"\"\"Возвращает рандомный токен, сэплируемый в соответствии с вероятностью встречаемости в тексте\"\"\"\n",
    "    tok_seq = [x[0] for x in seq]\n",
    "    probs =  [x[1] for x in seq]\n",
    "    probs = [x/sum(probs) for x in probs] \n",
    "    return np.random.choice(tok_seq, p=probs)\n",
    "\n",
    "def max_prob_token(seq, equal_choice=False):\n",
    "    \"\"\"Возвращает максимально вероятный токен.\n",
    "    equal_choice=True - возвращает рандомный максимально вероятный токен, если таких несколько\"\"\"\n",
    "    if not equal_choice:   \n",
    "        return sorted(seq, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    tok_seq = [x[0] for x in seq]\n",
    "    probs =  [x[1] for x in seq]\n",
    "    max_prob = max(probs)\n",
    "    max_prob_tokens = [x[0] for x in seq if x[1]==max_prob]\n",
    "    rnd_idx = np.random.randint(0,len(max_prob_tokens))\n",
    "    return max_prob_tokens[rnd_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, start_token=None, max_prob=False, equal_choice=False):\n",
    "    phrase = ''\n",
    "    t0, t1 = '$', '$'\n",
    "    start_flag = True\n",
    "    \n",
    "    while True:\n",
    "        if start_flag:\n",
    "            if start_token:  # вручную заданный стартовый токен\n",
    "                t1 = start_token.lower()\n",
    "            else:\n",
    "                t1 = rand_start_token(model[t0, t1])\n",
    "            start_flag = False\n",
    "        else:\n",
    "            if max_prob:   # сэмплируем максимально вероятный токен\n",
    "                t0, t1 = t1, max_prob_token(model[t0, t1], equal_choice)\n",
    "            else:          # сэмплируем случайный токен из распределения вероятнсти встречаемости в тексте\n",
    "                t0, t1 = t1, rand_prob_token(model[t0, t1])\n",
    "            \n",
    "        if t1 == '$': \n",
    "            break  # конец предложения - выход\n",
    " \n",
    "        if t1 in ('.!?,;:') or t0 == '$':\n",
    "            phrase += t1\n",
    "        else:\n",
    "            phrase += ' ' + t1\n",
    "            \n",
    "        if len(phrase.split( )) > 100: # для случая (max_prob=True, equal_choice=False)  - зачем это нужно? \n",
    "            break\n",
    "            \n",
    "    phrase = re.sub(\"--\", '-', phrase)\n",
    "    return phrase.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Давайте посмотрим, как это работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим текст \"Евгения Онегина\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"data/onegin.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### создадим модель на основе триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tg_pure = pure_trigram_model('data/onegin.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте попробуем сгенерировать преддожения, взяв первый максимально вероятный токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Онегин был, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence(model_tg_pure, start_token='онегин', max_prob=True, equal_choice=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вот почему мы при генерации выше ограничились 100 токенами!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте попробуем брать случайный токен из максимально вероятных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Онегин был готов со мною увидеть чуждые страны; но, развлечен пустым мечтаньем, я знаю: нежного парни перо не в силах я; для первых нет мне упований, а на полу мосье трике, догадливый поэт, умчал буянов пустякову, и, в деревне счастлив и рогат носил бы стеганый халат; узнал бы жизнь домашним кругом я ограничить захотел; когда б он знал довольно по-латыне, чтоб только слышать ваши речи, вам слово молвить, и, в деревне вс вам скучно, а досель порядка враг и расточитель, и, полно, милый, как будто бездна под ней чернеет и шумит.\n",
      "\n",
      "Онегин вновь часы считает, вновь ищу союза волшебных звуков, чувств и дум; пишу, что в высшем свете теперь являться я должна.\n",
      "\n",
      "Онегин был, что в высшем свете теперь являться я должна; что их бессмертная семья неотразимыми лучами, когда-нибудь, нас озарит и мир блаженством одарит.\n",
      "\n",
      "Онегин вновь часы считает, вновь обит, упрочен забвенью брошенный возок.\n",
      "\n",
      "Онегин вновь займуся им, большой потери в том порукой, супружество нам будет мукой.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sentence(model_tg_pure, start_token='онегин', max_prob=True, equal_choice=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посэмплируем следующий токен согласно вероятностному распределению триграмм в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Татьяна!\n",
      "\n",
      "Татьяна долго шла одна.\n",
      "\n",
      "Татьяна не дитя, учил его всему шутя, не все ли, дева красоты, слезу пролить над ранней урной и думать: он тем удвоил поэзии священный бред, петрарке шествуя вослед, а перед ним, и блеск в очах, с ней ни общих мнений, ни остротою, ни с чем не связанные сны, угрозы, моленья, клятвы, мнимый страх, записки на шести листах, обманы, сплетни, кольца, слезы, тайных мук отраду.\n",
      "\n",
      "Татьяна в руки не брала; про вести города, про моды беседы с прикрасой легкой клеветы.\n",
      "\n",
      "Татьяна русская душою, что завидно для поэта: во цвете радостных надежд, их не свершив еще для света, чуть помня их любовь и злость.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sentence(model_tg_pure, start_token='татьяна', max_prob=False, equal_choice=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самостоятельно реализуйте интерполяцию при оценке вероятности триграммы (см. лекцию)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерполированная оценка вероятности триграммы (w1,w2,w3) зависит от взвешенных слагаемых:\n",
    "- собственно вероятности данной триграммы - вес $\\lambda1$\n",
    "- вероятности биграммы (w2,w3) - вес $\\lambda2$\n",
    "- вероятности токена w3 - вес $\\lambda3$\n",
    "\n",
    "Для сглаживания возьмите следующие значения:\n",
    "- $\\lambda1$ = 0.9 \n",
    "- $\\lambda2$ = 0.095\n",
    "- $\\lambda3$ = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 0.9\n",
    "lambda2 = 0.095\n",
    "lambda3 = 0.005\n",
    "\n",
    "\n",
    "def interpolated_trigram_model(corpus):\n",
    "    lines = gen_lines(corpus)\n",
    "    tokens = gen_tokens(lines)\n",
    "    trigrams = gen_trigrams(tokens)\n",
    "    \n",
    "    (ug, bg, tg) = (defaultdict(lambda: 0.0) for i in range(3))\n",
    "\n",
    "    for t0, t1, t2 in trigrams:\n",
    "        ug[t0] += 1\n",
    "        bg[t0, t1] += 1\n",
    "        tg[t0, t1, t2] += 1\n",
    "\n",
    "    model = {}\n",
    "    for (t0, t1, t2), tg_cnt in tg.items():\n",
    "        \n",
    "        # just do it!\n",
    "        \n",
    "        tg_interpolated_prob = ...\n",
    "        ############################\n",
    "        \n",
    "        if (t0, t1) in model:\n",
    "            model[t0, t1].append((t2, tg_interpolated_prob))\n",
    "        else:\n",
    "            model[t0, t1] = [(t2, tg_interpolated_prob)]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Потестируем модель с интерполированной оценкой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tg_smooth = interpolated_trigram_model('data/onegin.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sentence(model_tg_pure, start_token='неосторожно', max_prob=True, equal_choice=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sentence(model_tg_smooth, start_token='неосторожно', max_prob=True, equal_choice=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиграйтесь с коэффициентами сглаживания и параметрами generate_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sentence(model_tg_smooth, max_prob=False, equal_choice=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Коллокации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим текст с произведениями Толстого, очистим его от тегов и разобъем на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tolstoy.txt\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_PATTERN = re.compile('<[^>]*>')\n",
    "content = re.sub(TAG_PATTERN, ' ', content)\n",
    "content_clean = re.sub(\"[^А-я]\",\" \", content.lower())  # only russian words\n",
    "content_words = re.sub('\\s+', ' ', content_clean).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделим частотные коллокации с помощью nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder_bg = nltk.BigramCollocationFinder.from_words(content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('иван', 'ильич'),\n",
       " ('князю', 'андрею'),\n",
       " ('анна', 'михайловна'),\n",
       " ('хаджи', 'мурат'),\n",
       " ('хаджи', 'мурата'),\n",
       " ('князя', 'андрея'),\n",
       " ('тех', 'пор'),\n",
       " ('марья', 'дмитриевна'),\n",
       " ('княжна', 'марья'),\n",
       " ('первый', 'раз')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder_bg.apply_freq_filter(100)  # встречающиеся более, чем 100 раз в корпусе\n",
    "finder_bg.nbest(bigram_measures.pmi, 10)  # 10 биграмм с максимальным PMI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Найти три самые частотные триграммные коллокации слов, встречающиеся в корпусе content_words более чем 20 раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Редакционное расстояние"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Встроенная библиотека difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher, get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "s = SequenceMatcher(None, \"выпьем пива\", \"выпьем чайку\")\n",
    "print(s.ratio()) # мера \"похожести\" двух строк (в диапазоне [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подробнее о том, что за алгоритм работает под капотом SequenceMatcher можно почитать [тут](https://docs.python.org/3/library/difflib.html#difflib.SequenceMatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пилка', 'пилат', 'сила']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['сила', 'пилат', 'силач', 'текила', 'косила', 'игла', 'пилка', 'пита',]\n",
    "query = 'пила'\n",
    "get_close_matches(query, words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Сравните расстояние Левенштейна и расстояние, возвращаемое SequenceMatcher, для пар слов (\"пила\", \"сила\") и (\"пила\",\"пилка\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !Найдите 3 ближайших слова по корпусу слов content_words (полученных в разделе с коллокациями) к слову \"пицца\" с помощью get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Библиотека jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jellyfish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной библиотеке поддерживается множество алгоритмов, например:\n",
    "- Расстояние Левенштейна\n",
    "- Расстояние Дамеру-Левенштейна \n",
    "- Расстояние Яро-Винклера \n",
    "- Расстояние Хэмминга\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(jellyfish.levenshtein_distance(u'jellyfish', u'smellyfihs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(jellyfish.damerau_levenshtein_distance(u'jellyfish', u'smellyfihs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Даны следующие пары слов:\n",
    "  [(\"baba\", \"arab\"), \n",
    "  (\"contest\", \"toner\"),\n",
    "  (\"eel\", \"lee\"),\n",
    "  (\"martial\", \"marital\"),\n",
    "  (\"monarchy\", \"democracy\"),\n",
    "  (\"seatback\", \"backseat\"),\n",
    "  (\"warfare\", \"farewell\"),\n",
    "  (\"smoking\", \"hospital\"),\n",
    "  (\"ape\", \"ea\")]\n",
    "\n",
    "\n",
    "Для каких из них расстояния Левененштейна и Дамерау-Левенштейна различаются? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### После выполнения заданий, ответьте на вопросы [тут](https://docs.google.com/forms/d/e/1FAIpQLSe5Je_L4a9H0qWKfLynViVZfDrT-TdiDfvZ93nfIv1rn_D5aQ/viewform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
