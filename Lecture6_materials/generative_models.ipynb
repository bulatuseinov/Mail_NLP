{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  _Лекция 1: Языковые модели, сравнение строк_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части ноутбука вы посмотрите, как рвботают простейшие генеративные языковые модели, основанные на триграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_alphabet = re.compile(u'[а-яА-Я0-9\\-]+|[.,:;?!]+') # clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вспомогательные ф-ии, возвращающие генераторы с линиями  и токенами загруженного корпуса текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lines(corpus):\n",
    "    data = open(corpus)\n",
    "    for line in data:\n",
    "        yield line.lower()\n",
    "\n",
    "def gen_tokens(lines):\n",
    "    for line in lines:\n",
    "        line = re.sub('\\.{2,3}', '\\.', line)\n",
    "        for token in r_alphabet.findall(line):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\.{2,3}\n"
     ]
    }
   ],
   "source": [
    "print('\\.{2,3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация словесных триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trigrams(tokens):\n",
    "    t0, t1 = '$', '$'\n",
    "    for t2 in tokens:\n",
    "        yield t0, t1, t2\n",
    "        if t2 in '.!?': # если это конец предложения\n",
    "            yield t1, t2, '$'\n",
    "            yield t2, '$','$'\n",
    "            t0, t1 = '$', '$' # начало следующего предложения\n",
    "        else:\n",
    "            t0, t1 = t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('$', '$', 'ab')\n",
      "('$', 'ab', '.')\n",
      "('ab', '.', '$')\n",
      "('.', '$', '$')\n",
      "('$', '$', 'cd')\n",
      "('$', 'cd', 'ef')\n"
     ]
    }
   ],
   "source": [
    "for i in gen_trigrams(['ab', '.', 'cd', 'ef']):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализуем простейшую триграммную генеративную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_trigram_model(corpus):\n",
    "    lines = gen_lines(corpus)\n",
    "    tokens = gen_tokens(lines)\n",
    "    trigrams = gen_trigrams(tokens)\n",
    "    \n",
    "    (bg, tg) = (defaultdict(lambda: 0.0) for i in range(2))\n",
    "\n",
    "    for t0, t1, t2 in trigrams:\n",
    "        bg[t0, t1] += 1\n",
    "        tg[t0, t1, t2] += 1\n",
    "\n",
    "    model = {}  # объект model - простейший словарь, хранящий вероятности следующего слова для каждой биграммы\n",
    "    ## Пример:\n",
    "    # model_tg_pure['онегин', 'был']\n",
    "    # Out: [(',', 0.5), ('готов', 0.5)]\n",
    "    \n",
    "    for (t0, t1, t2), tg_cnt in tg.items():\n",
    "        tg_prob = tg_cnt/bg[t0, t1]   #обратите внимание на оценку вероятности триграммы\n",
    "        if model.get((t0, t1)):\n",
    "            model[t0, t1].append((t2, tg_prob))  # (след. слово, вероятность)\n",
    "        else:\n",
    "            model[t0, t1] = [(t2, tg_prob)]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функции, возвращающие следующий токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_start_token(seq):\n",
    "    \"\"\"Возвращает рандомный стартовый токен, сэплируемый в соответствии с вероятностью встречаемости в тексте\"\"\"\n",
    "    seq_ = [x for x in seq if x[0].isalpha()]  # не будем стартовать с пунктуации\n",
    "    tok_seq, probs = [x[0] for x in seq_], [x[1] for x in seq_]\n",
    "    probs = [x/sum(probs) for x in probs]  # для np.random.choice сумма вероятностей должна суммироваться к 1 \n",
    "                                           #(нарушается, если выбросили пунктуационные токенв)\n",
    "    return np.random.choice(tok_seq, p=probs)\n",
    "\n",
    "def rand_prob_token(seq):\n",
    "    \"\"\"Возвращает рандомный токен, сэплируемый в соответствии с вероятностью встречаемости в тексте\"\"\"\n",
    "    tok_seq = [x[0] for x in seq]\n",
    "    probs =  [x[1] for x in seq]\n",
    "    probs = [x/sum(probs) for x in probs] \n",
    "    return np.random.choice(tok_seq, p=probs)\n",
    "\n",
    "def max_prob_token(seq, equal_choice=False):\n",
    "    \"\"\"Возвращает максимально вероятный токен.\n",
    "    equal_choice=True - возвращает рандомный максимально вероятный токен, если таких несколько\"\"\"\n",
    "    if not equal_choice:   \n",
    "        return sorted(seq, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    tok_seq = [x[0] for x in seq]\n",
    "    probs =  [x[1] for x in seq]\n",
    "    max_prob = max(probs)\n",
    "    max_prob_tokens = [x[0] for x in seq if x[1]==max_prob]\n",
    "    rnd_idx = np.random.randint(0,len(max_prob_tokens))\n",
    "    return max_prob_tokens[rnd_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, start_token=None, max_prob=False, equal_choice=False):\n",
    "    phrase = ''\n",
    "    t0, t1 = '$', '$'\n",
    "    start_flag = True\n",
    "    \n",
    "    while True:\n",
    "        if start_flag:\n",
    "            if start_token:  # вручную заданный стартовый токен\n",
    "                t1 = start_token.lower()\n",
    "            else:\n",
    "                t1 = rand_start_token(model[t0, t1])\n",
    "            start_flag = False\n",
    "        else:\n",
    "            if max_prob:   # сэмплируем максимально вероятный токен\n",
    "                t0, t1 = t1, max_prob_token(model[t0, t1], equal_choice)\n",
    "            else:          # сэмплируем случайный токен из распределения вероятнсти встречаемости в тексте\n",
    "                t0, t1 = t1, rand_prob_token(model[t0, t1])\n",
    "            \n",
    "        if t1 == '$': \n",
    "            break  # конец предложения - выход\n",
    " \n",
    "        if t1 in ('.!?,;:') or t0 == '$':\n",
    "            phrase += t1\n",
    "        else:\n",
    "            phrase += ' ' + t1\n",
    "            \n",
    "        if len(phrase.split( )) > 100: # для случая (max_prob=True, equal_choice=False)  - зачем это нужно? \n",
    "            break\n",
    "            \n",
    "    phrase = re.sub(\"--\", '-', phrase)\n",
    "    return phrase.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Давайте посмотрим, как это работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим текст \"Евгения Онегина\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"data/onegin.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### создадим модель на основе триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tg_pure = pure_trigram_model('data/onegin.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте попробуем сгенерировать преддожения, взяв первый максимально вероятный токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Онегин был, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence(model_tg_pure, start_token='онегин', max_prob=True, equal_choice=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вот почему мы при генерации выше ограничились 100 токенами!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Давайте попробуем брать случайный токен из максимально вероятных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Онегин, втайне усмехаясь, подходит к ольге взоры устремив, шептал: не знаю, ты нездорова; господь помилуй и спаси!\n",
      "\n",
      "Онегин был, друзья!\n",
      "\n",
      "Онегин, взорами сверкая, из-за стола гремя встает; все встали; он пьет одно стаканом красное вино; он пел поблеклый жизни цвет и вид в окно.\n",
      "\n",
      "Онегин был, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, сожаленьем, по крайней мере, звук речей казался иногда нежней, и, в деревне счастлив и рогат носил бы стеганый халат; узнал бы жизнь домашним кругом я ограничить захотел; когда б надежду я имела хоть редко, хоть и заглядывал я встарь в академический словарь.\n",
      "\n",
      "Онегин, скукой вновь гоним, близ него ее заметя, я думал уж о форме плана, и, может быть, еще чуднее: вот едет ленской на тройке чалых лошадей; давай обедать поскорей!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sentence(model_tg_pure, start_token='онегин', max_prob=True, equal_choice=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посэмплируем следующий токен согласно вероятностному распределению триграмм в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Татьяна на широкий луг всех маленьких ее подруг, она дрожала и бледнела.\n",
      "\n",
      "Татьяна видит с трепетаньем, какою мыслью, замечаньем бывал онегин поражен, в чем все уверены давно, моей татьяне вс равно.\n",
      "\n",
      "Татьяна с нетерпеньем, чтоб он не сделался поэтом, не отпирайтесь.\n",
      "\n",
      "Татьяна спит.\n",
      "\n",
      "Татьяна в оглавленье кратком находит азбучным порядком слова: бор, буря, цвет прекрасный увял на утренней заре пастух не гонит уж коров из хлева, и пустой.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sentence(model_tg_pure, start_token='татьяна', max_prob=False, equal_choice=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самостоятельно реализуйте интерполяцию при оценке вероятности триграммы (см. лекцию)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерполированная оценка вероятности триграммы (w1,w2,w3) зависит от взвешенных слагаемых:\n",
    "- собственно вероятности данной триграммы - вес $\\lambda1$\n",
    "- вероятности биграммы (w2,w3) - вес $\\lambda2$\n",
    "- вероятности токена w3 - вес $\\lambda3$\n",
    "\n",
    "Для сглаживания возьмите следующие значения:\n",
    "- $\\lambda1$ = 0.9 \n",
    "- $\\lambda2$ = 0.095\n",
    "- $\\lambda3$ = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 0.9\n",
    "lambda2 = 0.095\n",
    "lambda3 = 0.005\n",
    "\n",
    "\n",
    "def interpolated_trigram_model(corpus):\n",
    "    lines = gen_lines(corpus)\n",
    "    tokens = gen_tokens(lines)\n",
    "    \n",
    "    cnt = 0\n",
    "    for tok in tokens:\n",
    "        cnt += 1\n",
    "    \n",
    "    lines = gen_lines(corpus)\n",
    "    tokens = gen_tokens(lines)\n",
    "    trigrams = gen_trigrams(tokens)\n",
    "    (ug, bg, tg) = (defaultdict(lambda: 0.0) for i in range(3))\n",
    "\n",
    "    for t0, t1, t2 in trigrams:\n",
    "        ug[t0] += 1\n",
    "        bg[t0, t1] += 1\n",
    "        tg[t0, t1, t2] += 1\n",
    "\n",
    "    model = {}\n",
    "    \n",
    "    for (t0, t1, t2), tg_cnt in tg.items():\n",
    "        \n",
    "        # just do it!\n",
    "        \n",
    "        tg_interpolated_prob = lambda1* tg_cnt/bg[t0, t1] +  lambda2* tg_cnt/ug[t1] + lambda3 * tg_cnt/cnt\n",
    "        ############################\n",
    "        \n",
    "        if (t0, t1) in model:\n",
    "            model[t0, t1].append((t2, tg_interpolated_prob))\n",
    "        else:\n",
    "            model[t0, t1] = [(t2, tg_interpolated_prob)]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Потестируем модель с интерполированной оценкой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tg_smooth = interpolated_trigram_model('data/onegin.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Неосторожно, быть может, на солнце иней в день троицын, когда б я был от балов без ума: верней нет места для признаний и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence(model_tg_pure, start_token='неосторожно', max_prob=True, equal_choice=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Неосторожно, быть может, на солнце иней в день троицын, когда б я был от балов без ума: верней нет места для признаний и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и для ногтей и\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence(model_tg_smooth, start_token='неосторожно', max_prob=True, equal_choice=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой бедный ленской!\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentence(model_tg_smooth, start_token='мой', max_prob=True, equal_choice=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиграйтесь с коэффициентами сглаживания и параметрами generate_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начнем, пожалуй, - то верно б согласились вы, сны поэзии святой!\n",
      "Он из чувства, и за столом сидят чудовища кругом: один в рогах с собачьей мордой, другой поэт роскошным слогом живописал нам первый снег, звездами падая на брег.\n",
      "В какую бурю ощущений теперь он сердцем погружен!\n",
      "Но так и быть рукой пристрастной прими собранье пестрых глав, полусмешных, полупечальных, простонародных, идеальных, небрежный плод моих забав, напев торкватовых октав!\n",
      "Хоть не являла книга эта ни сладких вымыслов поэта, когда не в силах я; мне ваша искренность мила; она бежит, он говорил: дождусь ли дня?\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sentence(model_tg_smooth, max_prob=False, equal_choice=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Коллокации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим текст с произведениями Толстого, очистим его от тегов и разобъем на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tolstoy.txt\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_PATTERN = re.compile('<[^>]*>')\n",
    "content = re.sub(TAG_PATTERN, ' ', content)\n",
    "content_clean = re.sub(\"[^А-я]\",\" \", content.lower())  # only russian words\n",
    "content_words = re.sub('\\s+', ' ', content_clean).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделим частотные коллокации с помощью nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder_bg = nltk.BigramCollocationFinder.from_words(content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('иван', 'ильич'),\n",
       " ('князю', 'андрею'),\n",
       " ('анна', 'михайловна'),\n",
       " ('хаджи', 'мурат'),\n",
       " ('хаджи', 'мурата'),\n",
       " ('князя', 'андрея'),\n",
       " ('тех', 'пор'),\n",
       " ('марья', 'дмитриевна'),\n",
       " ('княжна', 'марья'),\n",
       " ('первый', 'раз')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder_bg.apply_freq_filter(100)  # встречающиеся более, чем 100 раз в корпусе\n",
    "finder_bg.nbest(bigram_measures.pmi, 10)  # 10 биграмм с максимальным PMI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Найти три самые частотные триграммные коллокации слов, встречающиеся в корпусе content_words более чем 20 раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT!\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder_tg = nltk.TrigramCollocationFinder.from_words(content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('граф', 'илья', 'андреич'), ('до', 'сих', 'пор'), ('по', 'крайней', 'мере')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder_tg.apply_freq_filter(20)\n",
    "finder_tg.nbest(trigram_measures.pmi, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Редакционное расстояние"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Встроенная библиотека difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher, get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "s = SequenceMatcher(None, \"выпьем пива\", \"выпьем чайку\")\n",
    "print(s.ratio()) # мера \"похожести\" двух строк (в диапазоне [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подробнее о том, что за алгоритм работает под капотом SequenceMatcher можно почитать [тут](https://docs.python.org/3/library/difflib.html#difflib.SequenceMatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пилка', 'пилат', 'сила']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['сила', 'пилат', 'силач', 'текила', 'косила', 'игла', 'пилка', 'пита',]\n",
    "query = 'пила'\n",
    "get_close_matches(query, words, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Сравните расстояние Левенштейна и расстояние, возвращаемое SequenceMatcher, для пар слов (\"пила\", \"сила\") и (\"пила\",\"пилка\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# DO IT!\n",
    "s = SequenceMatcher(None, \"пила\", \"сила\")\n",
    "print(s.ratio())\n",
    "\n",
    "s = SequenceMatcher(None, \"пила\", \"пилка\")\n",
    "print(s.ratio())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !Найдите 3 ближайших слова по корпусу слов content_words (полученных в разделе с коллокациями) к слову \"пицца\" с помощью get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['птица',\n",
       " 'птица',\n",
       " 'птица',\n",
       " 'птица',\n",
       " 'спицах',\n",
       " 'принца',\n",
       " 'принца',\n",
       " 'принца',\n",
       " 'принца',\n",
       " 'принца',\n",
       " 'принца',\n",
       " 'певица',\n",
       " 'спиц',\n",
       " 'пятница',\n",
       " 'пьяница',\n",
       " 'пьяница',\n",
       " 'птиц',\n",
       " 'птиц',\n",
       " 'птиц',\n",
       " 'птиц']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO IT!\n",
    "get_close_matches('пицца', content_words, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Библиотека jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jellyfish\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/3f/60ac86fb43dfbf976768e80674b5538e535f6eca5aa7806cf2fdfd63550f/jellyfish-0.6.1.tar.gz (132kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 998kB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jellyfish\n",
      "  Running setup.py bdist_wheel for jellyfish ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/alexander/Library/Caches/pip/wheels/9c/6f/33/92bb9a4b4562a60ba6a80cedbab8907e48bc7a8b1f369ea0ae\n",
      "Successfully built jellyfish\n",
      "Installing collected packages: jellyfish\n",
      "Successfully installed jellyfish-0.6.1\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jellyfish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной библиотеке поддерживается множество алгоритмов, например:\n",
    "- Расстояние Левенштейна\n",
    "- Расстояние Дамеру-Левенштейна \n",
    "- Расстояние Яро-Винклера \n",
    "- Расстояние Хэмминга\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(jellyfish.levenshtein_distance(u'jellyfish', u'smellyfihs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(jellyfish.damerau_levenshtein_distance(u'jellyfish', u'smellyfihs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Даны следующие пары слов:\n",
    "  [(\"baba\", \"arab\"), \n",
    "  (\"contest\", \"toner\"),\n",
    "  (\"eel\", \"lee\"),\n",
    "  (\"martial\", \"marital\"),\n",
    "  (\"monarchy\", \"democracy\"),\n",
    "  (\"seatback\", \"backseat\"),\n",
    "  (\"warfare\", \"farewell\"),\n",
    "  (\"smoking\", \"hospital\"),\n",
    "  (\"ape\", \"ea\")]\n",
    "\n",
    "\n",
    "Для каких из них расстояния Левененштейна и Дамерау-Левенштейна различаются? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# DO IT!\n",
    "print(jellyfish.levenshtein_distance(\"ape\", \"ea\"))\n",
    "print(jellyfish.damerau_levenshtein_distance(\"ape\", \"ea\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### После выполнения заданий, ответьте на вопросы [тут](https://docs.google.com/forms/d/e/1FAIpQLSe5Je_L4a9H0qWKfLynViVZfDrT-TdiDfvZ93nfIv1rn_D5aQ/viewform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
