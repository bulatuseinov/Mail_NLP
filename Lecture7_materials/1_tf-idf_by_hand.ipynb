{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log10\n",
    "from numpy import zeros, array, dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "## БОЛЬШЕ НИКАКИХ ИМПОРТОВ ДЕЛАТЬ НЕЛЬЗЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зададим небольшой корпус текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"о боже мама мама я схожу с ума\", \n",
    "          \"ее улыбка мама кругом голова\",\n",
    "          \"о боже мама мама пьяный без вина\", \n",
    "          \"ее улыбка мама самая самая\"]\n",
    "\n",
    "corpus = list(map(lambda x: x.split(), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['о', 'боже', 'мама', 'мама', 'я', 'схожу', 'с', 'ума'],\n",
       " ['ее', 'улыбка', 'мама', 'кругом', 'голова'],\n",
       " ['о', 'боже', 'мама', 'мама', 'пьяный', 'без', 'вина'],\n",
       " ['ее', 'улыбка', 'мама', 'самая', 'самая']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проиндексируем корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_corpus(corpus):\n",
    "    vocab_idx = {}\n",
    "    idx = 0\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            if word in vocab_idx:\n",
    "                continue\n",
    "            vocab_idx[word] = idx\n",
    "            idx += 1\n",
    "    return vocab_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'о': 0,\n",
       " 'боже': 1,\n",
       " 'мама': 2,\n",
       " 'я': 3,\n",
       " 'схожу': 4,\n",
       " 'с': 5,\n",
       " 'ума': 6,\n",
       " 'ее': 7,\n",
       " 'улыбка': 8,\n",
       " 'кругом': 9,\n",
       " 'голова': 10,\n",
       " 'пьяный': 11,\n",
       " 'без': 12,\n",
       " 'вина': 13,\n",
       " 'самая': 14}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_idx  = index_corpus(corpus) # map a token into id\n",
    "vocab_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "a[[1,2]] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_vectorizer(tokens, vocab_idx):\n",
    "    \"\"\"tokens - list of words\"\"\"\n",
    "    text_vector = zeros(len(vocab_idx))\n",
    "    \n",
    "    #SOME MAGIC HERE\n",
    "    for word in tokens:\n",
    "        text_vector[vocab_idx[word]] = 1\n",
    "    \n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print (binary_vectorizer(tokens, vocab_idx) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуйте СountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(tokens, vocab_idx):\n",
    "    \"\"\"tokens - list of words\"\"\"\n",
    "    text_vector = zeros(len(vocab_idx))\n",
    "    \n",
    "    # SOME MAGIC HERE\n",
    "    for word in tokens:\n",
    "        text_vector[vocab_idx[word]] += 1\n",
    "    \n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print (count_vectorizer(tokens, vocab_idx) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency\n",
    "TF  — это частотность термина, которая измеряет, насколько часто термин встречается в документе. В длинных документах термин может встретиться в больших количествах. Поэтому применяют относительные частоты — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в данном тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(tokens):\n",
    "    \"\"\"tokens - list of words\n",
    "    returns Counter with structure {word: tf}\"\"\"\n",
    "    \n",
    "    # SOME MAGIC HERE\n",
    "    tf_text = Counter()\n",
    "    \n",
    "    for word in tokens:\n",
    "        tf_text[word] += 1 \n",
    "        \n",
    "    return tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'самая': 2, 'ее': 1, 'улыбка': 1, 'мама': 1})\n"
     ]
    }
   ],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print( compute_tf(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency\n",
    "IDF — это обратная документная частота. Считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(tokens, corpus):\n",
    "    \"\"\"tokens - list of words\n",
    "    corpus - list of lists with words as elements\n",
    "    returns Counter with structure {word: idf}\"\"\"\n",
    "    idf_text = Counter()\n",
    "    \n",
    "    corp_len = len(corpus)\n",
    "    # SOME MAGIC HERE\n",
    "    for word in tokens:\n",
    "        cnt = 0\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                cnt += 1\n",
    "        idf_text[word] = log10(corp_len/cnt)\n",
    "    return idf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'самая': 0.6020599913279624, 'ее': 0.3010299956639812, 'улыбка': 0.3010299956639812, 'мама': 0.0})\n"
     ]
    }
   ],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print( compute_idf(tokens, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Скомбинируем и получим tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(corpus):\n",
    "    \"\"\"corpus - list of lists with words as elements\n",
    "    returns list of dicts {word: tf-idf} for each document in corpus\"\"\"\n",
    "    documents_list = []\n",
    "\n",
    "    for text in corpus:\n",
    "        computed_tf = compute_tf(text)\n",
    "        computed_idf = compute_idf(text, corpus)\n",
    "        tf_idf_dictionary = {}  # word: tf_idf_value\n",
    "        \n",
    "        # SOME MAGIC HERE\n",
    "        for word in text:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * computed_idf[word]    \n",
    "            \n",
    "        documents_list.append(tf_idf_dictionary)  # documents_list contains dict {word: tf_idf} for each sentence \n",
    "\n",
    "    return documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'о': 0.3010299956639812, 'боже': 0.3010299956639812, 'мама': 0.0, 'я': 0.6020599913279624, 'схожу': 0.6020599913279624, 'с': 0.6020599913279624, 'ума': 0.6020599913279624}\n",
      "{'ее': 0.3010299956639812, 'улыбка': 0.3010299956639812, 'мама': 0.0, 'кругом': 0.6020599913279624, 'голова': 0.6020599913279624}\n",
      "{'о': 0.3010299956639812, 'боже': 0.3010299956639812, 'мама': 0.0, 'пьяный': 0.6020599913279624, 'без': 0.6020599913279624, 'вина': 0.6020599913279624}\n",
      "{'ее': 0.3010299956639812, 'улыбка': 0.3010299956639812, 'мама': 0.0, 'самая': 1.2041199826559248}\n"
     ]
    }
   ],
   "source": [
    "for i in compute_tfidf(corpus):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самостоятельно сделаем  tf-idf векторизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(corpus, vocab_idx):\n",
    "    \"\"\"corpus - list of lists with words as elements\n",
    "    vocab_idx - map {word: idx} with indexes in vector space\n",
    "    returns list of lists with tf-idf vectors for each document\"\"\"\n",
    "    tf_idf_corpus = compute_tfidf(corpus)\n",
    "    corpus_vectorized = zeros((len(corpus), len(vocab_idx)))\n",
    "    \n",
    "    for i,sentence in enumerate(tf_idf_corpus):\n",
    "        for word in sentence:\n",
    "            \n",
    "            #SOME MAGIC HERE\n",
    "            corpus_vectorized[i, vocab_idx[word]] = sentence[word]\n",
    "\n",
    "    return corpus_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tfidf_vectorizer(corpus, vocab_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30103   , 0.30103   , 0.        , 0.60205999, 0.60205999,\n",
       "       0.60205999, 0.60205999, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Какие 2 вектора из получившихся ближе друг к другу по косинусному расстоянию: vec\\[0\\] и  vec\\[2\\] или  vec\\[1\\] и  vec\\[3\\]? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIST 02:  0.12598815766974242\n",
      "DIST 13:  0.14907119849998599\n"
     ]
    }
   ],
   "source": [
    "dist_02 = dot(vec[0], vec[2])/norm(vec[0])/norm([vec[2]])\n",
    "print('DIST 02: ', dist_02)\n",
    "\n",
    "dist_13 = dot(vec[1], vec[3])/norm(vec[1])/norm([vec[3]])\n",
    "print('DIST 13: ', dist_13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самостоятельно реализуем Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6 # Num of buckets for hashing\n",
    "def string2bucket(s):\n",
    "    return hash(s) % K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-9a281cb9c5e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примечание: достаточно провести хеширование токенов корпуса и результат подать на вход готовому tf-idf векторизатору\n",
    "\n",
    "P.S. Не забудьте, что словарь слово-индекс изменится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_corpus(corpus):\n",
    "    # ONE STRING OF CODE HERE\n",
    "    hashed_corpus = zeros(corp)\n",
    "    return hashed_corpus\n",
    "\n",
    "def hashing_vectorizer(corpus):\n",
    "    # 2-3 STRINGS OF CODE HERE\n",
    "    return hashed_corpus_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (hashing_vectorizer(corpus) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos02 = dot(vec[0],vec[2])/(norm(vec[0])*norm(vec[2]))\n",
    "cos13 = dot(vec[1],vec[3])/(norm(vec[1])*norm(vec[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos02, cos13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Какие 2 вектора из получившихся ближе друг к другу по косинусному расстоянию: vec\\[0\\] и  vec\\[2\\] или  vec\\[1\\] и  vec\\[3\\]? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте спросить у коллег, какие значения cos02 и cos13 получились у них, или перезапустите ноутбук еще раз и посмотрите на результаты (если вы - интроверт). \n",
    "\n",
    "Почему результаты различаются?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
