{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "Необходимо по тексту определить: лучше публиковать его в блоге на Хабрахабр или на Geektimes, другими словами нужно научить алгоритм отличать статьи одного блога от другого. Подразумевается, что текст технический и релевантен тематике данных блогов.\n",
    "\n",
    "В качестве исходных данных используются два json файла с 1000 текстами с каждого из этих двух сайтов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import RussianStemmer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_SEED = 0\n",
    "PATH_TO_DATA = \"data/habr/\"\n",
    "\n",
    "specSymb = {\"«\", \"»\", \"—\", \"“\", \"-\", \"№\"}\n",
    "specSymb = punctuation + \"«»—“-№\"\n",
    "pattern = re.compile(\"[\" + re.escape(specSymb) + \"]\")\n",
    "\n",
    "stopRus = stopwords.words('russian')\n",
    "stemmer_rus = SnowballStemmer(\"russian\")\n",
    "stemmer_eng = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(PATH_TO_DATA, \"geektimes.json\")) as f_in:\n",
    "    jsonka_geek = json.loads(f_in.read())\n",
    "\n",
    "df_geek = pd.DataFrame.from_dict(jsonka_geek)\n",
    "df_geek[\"label\"] = pd.Series([0]*len(df_geek))\n",
    "print(\"Size of geektimes sample: \", df_geek.shape[0])\n",
    "\n",
    "\n",
    "with open(join(PATH_TO_DATA, \"habrahabr.json\")) as f_in:\n",
    "    jsonka_habr = json.loads(f_in.read())\n",
    "\n",
    "df_habr = pd.DataFrame.from_dict(jsonka_habr)\n",
    "df_habr[\"label\"] = pd.Series([1]*len(df_habr))\n",
    "print(\"Size of habrahabr sample: \", df_habr.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_habr, df_geek])\n",
    "df_all = df_all.reset_index()[[\"title\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(pattern, '', text)\n",
    "    text = text.replace('ё', 'е')\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_without_nums = filter(lambda x: not x.isdigit(), tokens)  # skip all numbers\n",
    "    tokens_without_stop = filter(lambda x: x not in stopRus, tokens_without_nums)\n",
    "    tokens_stem_rus = map(lambda x: stemmer_rus.stem(x), tokens_without_stop)\n",
    "    tokens_stem_eng = map(lambda x: stemmer_eng.stem(x), tokens_stem_rus)\n",
    "    \n",
    "    return \" \".join(tokens_stem_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_all[\"title\"] = df_all[\"title\"].apply(preprocess)\n",
    "df_all[\"text\"] = df_all[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all[\"title\"].str.cat(pd.Series([' ']*len(df_all))).str.cat(df_all[\"text\"])\n",
    "y = df_all[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = RND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0], X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define vectorizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(binary=True)\n",
    "# попробуйте разные типы векторизаторов бинарный, CountVectorizer, TfIdfVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1, solver=\"liblinear\")  # посмотрите, как влияет коэффициент и тип регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc(y_score, y_test):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_score)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    f1 = plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc, linewidth=3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, X_test, y_train, y_test, model, vectorizer):\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "\n",
    "    print(\"precision: %s;\\nrecall %s\" % (precision, recall))\n",
    "    \n",
    "    y_score = model.decision_function(X_test)\n",
    "    plot_roc_auc(y_score, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_model(X_train, X_test, y_train, y_test, logreg, vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prediction for new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(inputTextFile, vectorizer, model):\n",
    "    \"\"\"Perform prediction for inputTextFile: on what site this text shoulb be published - GeekTimes or Habrahabr.\"\"\"\n",
    "    readFile = open(inputTextFile, encoding='utf8').read()\n",
    "    t = \" \".join(readFile.split())\n",
    "    proc = preprocess(t)\n",
    "    vectorize = vectorizer.transform([proc])\n",
    "    flag = (model.predict_proba(vectorize)[0] == max(model.predict_proba(vectorize)[0]))[0]\n",
    "    blog = flag and u\"GeekTimes\" or u\"Habrahabr\"\n",
    "    print (\"C вероятностью {0} % данный текст опубликован на\".format(round(max(model.predict_proba(vectorize)[0])*100,2)), blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getPrediction(\"data/habr/input.txt\", vect, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Попробуем vowpal wabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare sample for vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(PATH_TO_DATA, \"geek_habr_sample_train.vw\"), \"w\") as f_out:\n",
    "    for idx in X_train.index:\n",
    "        row = df_all.loc[idx]\n",
    "        label = row.label\n",
    "        title = row.title\n",
    "        text = row.text\n",
    "        \n",
    "        if label == 0:\n",
    "            label = -1 ## AS VW EXSPECTS\n",
    "            \n",
    "        line = str(label) + \" |title \" + title + \" |text \" + text + \"\\n\"\n",
    "        f_out.write(line)\n",
    "        \n",
    "        \n",
    "with open(join(PATH_TO_DATA, \"geek_habr_sample_test.vw\"), \"w\") as f_out:\n",
    "    for idx in X_test.index:\n",
    "        row = df_all.loc[idx]\n",
    "        label = row.label\n",
    "        title = row.title\n",
    "        text = row.text\n",
    "        \n",
    "        if label == 0:\n",
    "            label = -1 ## AS VW EXSPECTS\n",
    "            \n",
    "        line = str(label) + \" |title \" + title + \" |text \" + text + \"\\n\"\n",
    "        f_out.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn with vowpal wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -d data/habr/geek_habr_sample_train.vw --loss_function logistic -f data/habr/vw_trained.model \\\n",
    "                    --ngram 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -i data/habr/vw_trained.model -t -d data/habr/geek_habr_sample_test.vw \\\n",
    "                                        -p data/habr/vw_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/habr/vw_predictions.txt') as pred_file:\n",
    "    y_score_vw = [float(label)\n",
    "                             for label in pred_file.readlines()]\n",
    "\n",
    "\n",
    "plot_roc_auc(y_score_vw, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(y_test, y_score_vw)\n",
    "recall = metrics.recall_score(y_test, y_score_vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_vw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "y_score_vw_normalized = sigmoid(np.array(y_score_vw))\n",
    "y_score_vw_normalized = y_score_vw_normalized > 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(y_test, y_score_vw_normalized)\n",
    "recall = metrics.recall_score(y_test, y_score_vw_normalized)\n",
    "print(\"precision: %s;\\nrecall %s\" % (precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
