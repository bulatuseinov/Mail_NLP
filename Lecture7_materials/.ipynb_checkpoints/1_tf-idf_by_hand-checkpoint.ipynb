{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log10\n",
    "from numpy import zeros, array, dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "## БОЛЬШЕ НИКАКИХ ИМПОРТОВ ДЕЛАТЬ НЕЛЬЗЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зададим небольшой корпус текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"о боже мама мама я схожу с ума\", \n",
    "          \"ее улыбка мама кругом голова\",\n",
    "          \"о боже мама мама пьяный без вина\", \n",
    "          \"ее улыбка мама самая самая\"]\n",
    "\n",
    "corpus = list(map(lambda x: x.split(), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['о', 'боже', 'мама', 'мама', 'я', 'схожу', 'с', 'ума'],\n",
       " ['ее', 'улыбка', 'мама', 'кругом', 'голова'],\n",
       " ['о', 'боже', 'мама', 'мама', 'пьяный', 'без', 'вина'],\n",
       " ['ее', 'улыбка', 'мама', 'самая', 'самая']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проиндексируем корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_corpus(corpus):\n",
    "    vocab_idx = {}\n",
    "    idx = 0\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            if word in vocab_idx:\n",
    "                continue\n",
    "            vocab_idx[word] = idx\n",
    "            idx+=1\n",
    "    return vocab_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'без': 12,\n",
       " 'боже': 1,\n",
       " 'вина': 13,\n",
       " 'голова': 10,\n",
       " 'ее': 7,\n",
       " 'кругом': 9,\n",
       " 'мама': 2,\n",
       " 'о': 0,\n",
       " 'пьяный': 11,\n",
       " 'с': 5,\n",
       " 'самая': 14,\n",
       " 'схожу': 4,\n",
       " 'улыбка': 8,\n",
       " 'ума': 6,\n",
       " 'я': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_idx  = index_corpus(corpus) # map a token into id\n",
    "vocab_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_vectorizer(tokens, vocab_idx):\n",
    "    \"\"\"tokens - list of words\"\"\"\n",
    "    text_vector = zeros(len(vocab_idx))\n",
    "    \n",
    "    #SOME MAGIC HERE\n",
    "    \n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print (binary_vectorizer(tokens, vocab_idx) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуйте СountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(tokens, vocab_idx):\n",
    "    \"\"\"tokens - list of words\"\"\"\n",
    "    text_vector = zeros(len(vocab_idx))\n",
    "    \n",
    "    # SOME MAGIC HERE\n",
    "    \n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print (count_vectorizer(tokens, vocab_idx) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency\n",
    "TF  — это частотность термина, которая измеряет, насколько часто термин встречается в документе. В длинных документах термин может встретиться в больших количествах. Поэтому применяют относительные частоты — делят количество раз, когда нужный термин встретился в тексте, на общее количество слов в данном тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(tokens):\n",
    "    \"\"\"tokens - list of words\n",
    "    returns Counter with structure {word: tf}\"\"\"\n",
    "    \n",
    "    # SOME MAGIC HERE\n",
    "        \n",
    "    return tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print( compute_tf(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency\n",
    "IDF — это обратная документная частота. Считается как логарифм от общего количества документов, делённого на количество документов, в которых встречается термин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(tokens, corpus):\n",
    "    \"\"\"tokens - list of words\n",
    "    corpus - list of lists with words as elements\n",
    "    returns Counter with structure {word: idf}\"\"\"\n",
    "    idf_text = Counter()\n",
    "    \n",
    "    # SOME MAGIC HERE\n",
    "    \n",
    "    return idf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ее улыбка мама самая самая\"\n",
    "tokens = text.split()\n",
    "print( compute_idf(tokens, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Скомбинируем и получим tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(corpus):\n",
    "    \"\"\"corpus - list of lists with words as elements\n",
    "    returns list of dicts {word: tf-idf} for each document in corpus\"\"\"\n",
    "    documents_list = []\n",
    "\n",
    "    for text in corpus:\n",
    "        computed_tf = compute_tf(text)\n",
    "        computed_idf = compute_idf(text, corpus)\n",
    "        tf_idf_dictionary = {}  # word: tf_idf_value\n",
    "        \n",
    "        # SOME MAGIC HERE    \n",
    "            \n",
    "        documents_list.append(tf_idf_dictionary)  # documents_list contains dict {word: tf_idf} for each sentence \n",
    "\n",
    "    return documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in compute_tfidf(corpus):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самостоятельно сделаем  tf-idf векторизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(corpus, vocab_idx):\n",
    "    \"\"\"corpus - list of lists with words as elements\n",
    "    vocab_idx - map {word: idx} with indexes in vector space\n",
    "    returns list of lists with tf-idf vectors for each document\"\"\"\n",
    "    tf_idf_corpus = compute_tfidf(corpus)\n",
    "    corpus_vectorized = zeros((len(corpus), len(vocab_idx)))\n",
    "    \n",
    "    for i,sentence in enumerate(tf_idf_corpus):\n",
    "        for word in sentence:\n",
    "            \n",
    "            #SOME MAGIC HERE\n",
    "\n",
    "    return corpus_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = tfidf_vectorizer(corpus, vocab_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Какие 2 вектора из получившихся ближе друг к другу по косинусному расстоянию: vec\\[0\\] и  vec\\[2\\] или  vec\\[1\\] и  vec\\[3\\]? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самостоятельно реализуем Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6 # Num of buckets for hashing\n",
    "def string2bucket(s):\n",
    "    return hash(s) % K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примечание: достаточно провести хеширование токенов корпуса и результат подать на вход готовому tf-idf векторизатору\n",
    "\n",
    "P.S. Не забудьте, что словарь слово-индекс изменится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_corpus(corpus):\n",
    "    # ONE STRING OF CODE HERE\n",
    "    return hashed_corpus\n",
    "\n",
    "def hashing_vectorizer(corpus):\n",
    "    # 2-3 STRINGS OF CODE HERE\n",
    "    return hashed_corpus_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (hashing_vectorizer(corpus) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos02 = dot(vec[0],vec[2])/(norm(vec[0])*norm(vec[2]))\n",
    "cos13 = dot(vec[1],vec[3])/(norm(vec[1])*norm(vec[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos02, cos13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Какие 2 вектора из получившихся ближе друг к другу по косинусному расстоянию: vec\\[0\\] и  vec\\[2\\] или  vec\\[1\\] и  vec\\[3\\]? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте спросить у коллег, какие значения cos02 и cos13 получились у них, или перезапустите ноутбук еще раз и посмотрите на результаты (если вы - интроверт). \n",
    "\n",
    "Почему результаты различаются?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
