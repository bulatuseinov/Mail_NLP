{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1: LSH from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mmh3\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/7e/3ddcab0a9fcea034212c02eb411433db9330e34d626360b97333368b4052/mmh3-2.5.1.tar.gz\n",
      "Building wheels for collected packages: mmh3\n",
      "  Running setup.py bdist_wheel for mmh3 ... \u001b[?25lerror\n",
      "  Complete output from command /Users/alexander/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/2k/shj0p2n928j7fjq528nlws040000gn/T/pip-install-7h31p_dn/mmh3/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /private/var/folders/2k/shj0p2n928j7fjq528nlws040000gn/T/pip-wheel-hbti4yle --python-tag cp36:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'mmh3' extension\n",
      "  creating build\n",
      "  creating build/temp.macosx-10.7-x86_64-3.6\n",
      "  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include/python3.6m -c mmh3module.cpp -o build/temp.macosx-10.7-x86_64-3.6/mmh3module.o\n",
      "  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "  1 warning generated.\n",
      "  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include/python3.6m -c MurmurHash3.cpp -o build/temp.macosx-10.7-x86_64-3.6/MurmurHash3.o\n",
      "  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "  1 warning generated.\n",
      "  creating build/lib.macosx-10.7-x86_64-3.6\n",
      "  g++ -bundle -undefined dynamic_lookup -L/Users/alexander/anaconda3/lib -arch x86_64 -L/Users/alexander/anaconda3/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/mmh3module.o build/temp.macosx-10.7-x86_64-3.6/MurmurHash3.o -o build/lib.macosx-10.7-x86_64-3.6/mmh3.cpython-36m-darwin.so\n",
      "  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "  ld: library not found for -lstdc++\n",
      "  clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  error: command 'g++' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for mmh3\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for mmh3\n",
      "Failed to build mmh3\n",
      "Installing collected packages: mmh3\n",
      "  Running setup.py install for mmh3 ... \u001b[?25lerror\n",
      "    Complete output from command /Users/alexander/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/2k/shj0p2n928j7fjq528nlws040000gn/T/pip-install-7h31p_dn/mmh3/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/2k/shj0p2n928j7fjq528nlws040000gn/T/pip-record-jo79jddx/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'mmh3' extension\n",
      "    creating build\n",
      "    creating build/temp.macosx-10.7-x86_64-3.6\n",
      "    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include/python3.6m -c mmh3module.cpp -o build/temp.macosx-10.7-x86_64-3.6/mmh3module.o\n",
      "    warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "    1 warning generated.\n",
      "    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include -arch x86_64 -I/Users/alexander/anaconda3/include/python3.6m -c MurmurHash3.cpp -o build/temp.macosx-10.7-x86_64-3.6/MurmurHash3.o\n",
      "    warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "    1 warning generated.\n",
      "    creating build/lib.macosx-10.7-x86_64-3.6\n",
      "    g++ -bundle -undefined dynamic_lookup -L/Users/alexander/anaconda3/lib -arch x86_64 -L/Users/alexander/anaconda3/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-3.6/mmh3module.o build/temp.macosx-10.7-x86_64-3.6/MurmurHash3.o -o build/lib.macosx-10.7-x86_64-3.6/mmh3.cpython-36m-darwin.so\n",
      "    clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "    ld: library not found for -lstdc++\n",
      "    clang: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "    error: command 'g++' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/Users/alexander/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/2k/shj0p2n928j7fjq528nlws040000gn/T/pip-install-7h31p_dn/mmh3/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/2k/shj0p2n928j7fjq528nlws040000gn/T/pip-record-jo79jddx/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/2k/shj0p2n928j7fjq528nlws040000gn/T/pip-install-7h31p_dn/mmh3/\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmh3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0dba3105282a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmh3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhash\u001b[0m  \u001b[0;31m# murmur hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmh3'"
     ]
    }
   ],
   "source": [
    "from mmh3 import hash  # murmur hash \n",
    "from random import randint\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зададим игрушечный корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "          \"Who was the first king of Poland\",\n",
    "          \"Who was the first ruler of Poland\",\n",
    "          \"Who was the last pharaoh of Egypt\", \n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем функцию шинглирования корпуса.\n",
    "\n",
    "Шинглирование может происходит по словам и по словесным нграммам, также добавим возможность хешировать на лету слова/нграммы. Это позволит работать со словами вне словаря и сократить размер признакового пространства. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingling(corpus, k=0, hash_shingles=False, buckets=2**15):\n",
    "    # corpus = list of strings\n",
    "    # k - size of char-ngrams for shingling\n",
    "    # hash_shingles - use shingle hashing or not\n",
    "    # buckets - num of buckets for hashing if hash_shingles=True\n",
    "    \n",
    "    if k:\n",
    "        shingled_corpus = [[phrase[i:i+k] for i in range(len(phrase) - k+1) ] for phrase in corpus]\n",
    "    \n",
    "    else: #if k = 0 - use words as shingles\n",
    "        shingled_corpus = [phrase.split() for phrase in corpus]\n",
    "    \n",
    "    if hash_shingles: \n",
    "        shingled_corpus = [[hash(shingle, signed=False) % buckets for shingle in doc] for doc in shingled_corpus]\n",
    "    \n",
    "    return shingled_corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingled_corpus = shingling(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingled_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, считающую коэффициент Жаккара между документами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(l1, l2):\n",
    "    # l1, l2 - lists of tokens\n",
    "    s1 = set(l1)\n",
    "    s2 = set(l2)\n",
    "    \n",
    "    intersection_size = len(s1.intersection(s2))\n",
    "    union_size = len(s1.union(s2))\n",
    "    \n",
    "    return intersection_size/union_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1, doc2 = shingled_corpus[:2]\n",
    "jaccard_sim(doc1, doc2)  # как и следовало ожидать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Зададим функцию, генерирующую минхэш-сигнатуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minhash_signature(doc, num_perm=100):\n",
    "    # doc - list of shinles\n",
    "    # num_perm - number of hash functions to use\n",
    "    \n",
    "    minhash_signature = []\n",
    "\n",
    "    for i in range(num_perm):\n",
    "        new_hash = lambda x: hash(str(x), signed=False) % (100000+i)  # хэш-функции должны быть разными\n",
    "        minhash = min([new_hash(shingle) for shingle in doc])  # take only minimum of hashed values\n",
    "        minhash_signature.append(minhash)\n",
    "        \n",
    "    return minhash_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сгенерировать минхэш-сигнатуру для первого и второго документа в коллекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhash_size = 12  # 12 hash functions to use\n",
    " \n",
    "minhash_doc0 = generate_minhash_signature(shingled_corpus[0], mhash_size)  \n",
    "minhash_doc1 = generate_minhash_signature(shingled_corpus[1], mhash_size)\n",
    "\n",
    "print(minhash_doc0)\n",
    "print(minhash_doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что здесь считать коэффициент Жаккара нужно, сравнивая элементы попарно, а не мешая их в одно множество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim_mhash(l1, l2):\n",
    "    # l1, l2 - lists of minhashes\n",
    "    l1 = np.array(l1)\n",
    "    l2 = np.array(l2)\n",
    "    \n",
    "    intersection = (l1 == l2).sum()\n",
    "    \n",
    "    return intersection/l1.size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_sim_mhash(minhash_doc0, minhash_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Примерно оценить размер группы и количество групп можно, исходя из того порога коэффициента Жаккара, который мы зададим для \"похожих\" документов\n",
    "\n",
    "Будем считать \"похожими\" документы с коэффициентов Жаккара, большим 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhash_size = 12\n",
    "n = 4\n",
    "b = mhash_size/n #  количество групп\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При выбранных $n$ и $b$ порог должен быть примерно равен ${1\\over b}^{1\\over n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/b)**(1/n) #  больше 0.7 - значит группы, размера 4 подойдут"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь опишем функцию, разбивающую сигнатуры по группам "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supershingles(signature, n=4):\n",
    "    # signature - list of minhashes\n",
    "    return [signature[i:i+n] for i in range(0,len(signature),n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minhash_doc0)\n",
    "print(get_supershingles(minhash_doc0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь каждая группа - ключ, по которому можно искать похожие документы в хеш-таблице\n",
    "\n",
    "При наличии хотя бы одной совпадающей группы минхэшей в запросе и обработанном корпусе, запрос и соответствующий документ считаются потенциально похожей парой документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте заведем такую хеш-таблицу, которая будет хранить информацию о корпусе.\n",
    "Заметим, что строго говоря, группа минхешей **неинвариантна относительно местонахождения**\n",
    "\n",
    "\n",
    "Проще на примере:\n",
    "- l1 =  (  [2,1,1]   [3,3,4]   )\n",
    "- l2 =  (  [1,2,3]   [2,1,1]   )\n",
    "\n",
    "Группа [2,1,1] есть в обоих сигнатурах, но помним, что сранивать сигнатуры в данном случае нужно попарно между собой. Таким образом общая группа в одном и в другом случаях отражает разные преобразования входных данных.\n",
    "\n",
    "Попарное сравнение - дает коэффициент Жаккара = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так что будем при формировании хеш-таблицы учитывать порядковый номер следования группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingled_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "params = {'mhash_size': 12, \"n\":4, \"k_shingles\":0, \"hash_shingles\":False, \"buckets\":2**15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_training(corpus, parameters):\n",
    "    \"\"\"shingled_corpus - list of lists with documents' shingles\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    lsh_dict = {}\n",
    "    \n",
    "    shingled_corpus = shingling(corpus, parameters[\"k_shingles\"], \n",
    "                                parameters[\"hash_shingles\"], \n",
    "                                parameters[\"buckets\"])\n",
    "    \n",
    "    for doc_id, doc in enumerate(shingled_corpus):\n",
    "        minhash_signature = generate_minhash_signature(doc, num_perm=parameters[\"mhash_size\"])\n",
    "        minhash_groups = get_supershingles(minhash_signature, n=parameters[\"n\"])\n",
    "        \n",
    "        for i,group in enumerate(minhash_groups):\n",
    "            hash_key = hash(str(group),signed=False) % (100000+i)   # превращаем список с группой в строку, хешируем с учетом порядекового номера группы в сигнатуре \n",
    "            lsh_dict[hash_key] = corpus[doc_id]\n",
    "    \n",
    "    return lsh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_words = lsh_training(corpus, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Допустим теперь есть запрос и мы хотим найти максимально похожие документы в базе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для запроса нужно провести все те же операции, чято и для документов в базе. Заметим, что набор хэш-функций **обязан остаться тем же самым**. Иначе все просто бессмысленно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(query, lsh_dict, parameters):\n",
    "    # query - document as string\n",
    "    possible_duplicates = set()\n",
    "    query = [query]\n",
    "    shingled_query = shingling(query, parameters[\"k_shingles\"], \n",
    "                                parameters[\"hash_shingles\"], \n",
    "                                parameters[\"buckets\"])  # параметры шинглирования должны быть те же, что и для корпуса\n",
    "    \n",
    "    query_signature = generate_minhash_signature(shingled_query[0], parameters[\"mhash_size\"])\n",
    "    query_groups = get_supershingles(query_signature, n=parameters[\"n\"])\n",
    "    \n",
    "    for i,group in enumerate(query_groups):\n",
    "        hash_key = hash(str(group),signed=False) % (100000+i)\n",
    "        if lsh_dict.get(hash_key):\n",
    "            possible_duplicates.add(lsh_dict[hash_key])\n",
    "\n",
    "    if possible_duplicates:\n",
    "        print(\"Possible duplicates for query:\")\n",
    "        for doc in possible_duplicates:\n",
    "            print(doc)\n",
    "            \n",
    "    else:\n",
    "        print(\"Duplicates for query not founded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Who was the first king of\", lsh_words, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Who was the first king of Polan\", lsh_words, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Who was the last\", lsh_words, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шинглирование по нграммам помогает бороться с ошибками и опечатками в запросе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "params_ngrams = {'mhash_size': 12, \"n\":3, \"k_shingles\":5, \"hash_shingles\":False, \"buckets\":2**15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_ngrams = lsh_training(corpus, params_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Who was the first king of Polan\", lsh_ngrams, params_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"Who was the first king of Poand\", lsh_ngrams, params_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"the first king of Poland\", lsh_ngrams, params_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simhash - алгоритм, альтернативный минхэшу - превратить последовательность шинглов в сигнатуру (только сигнатура получатется немного по другому). \n",
    "\n",
    "После получения сигнатуры выполняется LSH: разбиение сигнатуры на супершинглы, строится таблица дубликатов\n",
    "\n",
    "Основная идея состоит в том, что для каждого шингла формируются n-битные бинарные хеши, которые затем особым образом складываются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция, получающая от шингла n_bit-ный бинарный хеш"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_hash(s, n_bit=8):\n",
    "    binary_hash = (\"{0:0%sb}\"%str(n_bit)).format(hash(s, signed=False)%(2**n_bit-1))\n",
    "    return binary_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_binary_hash(\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция, получающая из документа SimHash сигнатуру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- получаем для шинглов обычный 32-битный integer хеш\n",
    "- хешируем его на **$2^{n\\_bit}-1$** bucket'ов, чтобы получить n_bit-ный бинарный хеш  \n",
    "- затем в бинарных хешах превращаем все 0 в -1\n",
    "- затем взвешиваем бинарные хеши для каждого шингла в соответствии с term-frequency для данного шингла (опционально)\n",
    "- затем побитово суммируем все хеши для каждого шингла\n",
    "- затем бинаризуем побитово суммарный хеш: если компонент > 0 - ставим 1, иначе - ставим 0\n",
    "- полученная сигнатура и есть SimHash сигнатура документа "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simhash(doc, n_bit=8):\n",
    "    # doc - list of shingles    \n",
    "    hashes = [get_binary_hash(shingle, n_bit=n_bit) for shingle in doc] # get binary hashes for all shingles\n",
    "    hashes = np.array([[1 if bit == \"1\" else -1 for bit in bithash] for bithash in hashes])   # map 0 to -1 \n",
    "    simhash = np.array([1 if i>0 else 0 for i in hashes.sum(axis=0)]) # sum up all hashes bitwise and binarize final hash\n",
    "\n",
    "    return simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simhash_signature = get_simhash(doc)\n",
    "simhash_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_supershingles(simhash_signature, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Функция близости между векторами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(simhash1, simhash2):\n",
    "    # FYI: Hamming distance for bit vectors = number of different components for pair of vectors\n",
    "    # Hamming([1,0,1,0,1], [1,0,1,0,0]) = 1 - only last bit differs\n",
    "    return (simhash1 == simhash2).sum() / len(simhash1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какое сходство между первыми двумя документами корпуса?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = corpus[0].split()\n",
    "simhash_signature1 = get_simhash(doc1)\n",
    "\n",
    "doc2 = corpus[1].split()\n",
    "simhash_signature2 = get_simhash(doc2)\n",
    "\n",
    "get_similarity(simhash_signature1, simhash_signature2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Они очень похожи с точки зрения Симхэша, даже более, чем с точки зрения Минхэш сигнатуры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Давайте сделаем LSH с SimHash сигнатурой и посмотрим, что получится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "params = {\"n_bits\": 16, \"n\":4, \"k_shingles\":0, \"hash_shingles\":False, \"buckets\":2**15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_training(corpus, parameters):\n",
    "    \"\"\"shingled_corpus - list of lists with documents' shingles\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    lsh_dict = {}\n",
    "    \n",
    "    shingled_corpus = shingling(corpus, parameters[\"k_shingles\"], \n",
    "                                parameters[\"hash_shingles\"], \n",
    "                                parameters[\"buckets\"])\n",
    "    \n",
    "    for doc_id, doc in enumerate(shingled_corpus):\n",
    "        simhash_signature = get_simhash(doc, n_bit=parameters[\"n_bits\"])\n",
    "        simhash_groups = get_supershingles(simhash_signature, n=parameters[\"n\"])\n",
    "        \n",
    "        for i,group in enumerate(simhash_groups):\n",
    "            hash_key = hash(str(group),signed=False) % (100000+i)   # превращаем список с группой в строку, хешируем с учетом порядекового номера группы в сигнатуре \n",
    "            lsh_dict[hash_key] = corpus[doc_id]\n",
    "    \n",
    "    return lsh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(query, lsh_dict, parameters):\n",
    "    # query - document as string\n",
    "    possible_duplicates = set()\n",
    "    query = [query]\n",
    "    shingled_query = shingling(query, parameters[\"k_shingles\"], \n",
    "                                parameters[\"hash_shingles\"], \n",
    "                                parameters[\"buckets\"])  # параметры шинглирования должны быть те же, что и для корпуса\n",
    "    \n",
    "    query_signature = get_simhash(shingled_query[0], parameters[\"n_bits\"])\n",
    "    query_groups = get_supershingles(query_signature, n=parameters[\"n\"])\n",
    "    \n",
    "    for i,group in enumerate(query_groups):\n",
    "        hash_key = hash(str(group),signed=False) % (100000+i)\n",
    "        if lsh_dict.get(hash_key):\n",
    "            possible_duplicates.add(lsh_dict[hash_key])\n",
    "\n",
    "    if possible_duplicates:\n",
    "        print(\"Possible duplicates for query:\")\n",
    "        for doc in possible_duplicates:\n",
    "            print(doc)\n",
    "            \n",
    "    else:\n",
    "        print(\"Duplicates for query not founded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_simhash_words = lsh_training(corpus, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_simhash_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"the first king of Poad\", lsh_simhash_words, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"the last pharaoh of Eypt\", lsh_simhash_words, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание:\n",
    "\n",
    "1) Реализуйте взвешивание бинарных хешей для шинглов с помощью TF(не надо нормализовывать на длину документа). Так как для данного корпуса это бессмысленно, возьмите корпус dorn_corpus с прошлого занятия\n",
    "\n",
    "2) Сделайте ф-ии lsh_training и find_similar универсальными, чтобы тип вычисления сигнатуры (MinHash, SimHash) можно было задавать параметром"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dorn_corpus = [\"о боже мама мама я схожу с ума\", \n",
    "              \"ее улыбка мама кругом голова\",\n",
    "              \"о боже мама мама пьяный без вина\", \n",
    "              \"ее улыбка мама самая самая\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2: LSH - Quora queries similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Воспользуемся готовой реализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from random import randint\n",
    "from datasketch import MinHash, MinHashLSHForest\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теже самые функции шинглирования для текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\n",
    "def shingling(text, k=7, hash_shingles=False, buckets=2**15):\n",
    "    text = text.lower()\n",
    "    if k:\n",
    "        shingled_text = [text[i:i+k] for i in range(len(text) - k+1) ]\n",
    "        \n",
    "    else: #if k = 0 - use words as shingles\n",
    "        shingled_text = text.split()\n",
    "    \n",
    "    if hash_shingles: \n",
    "        shingled_text = [hash(shingle, signed=False) % buckets for shingle in shingled_text]\n",
    "    \n",
    "    return shingled_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The devil went down to Georgia'\n",
    "print('The shingles (tokens) are:', shingling(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest(corpus, perms):\n",
    "    \"\"\"\n",
    "    corpus - list of docs (each doc is string)\n",
    "    perms - number of permutations / of number of hash functions to use\n",
    "    returns LSH forest - more effective data structure for duplicates searching\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    minhash = []\n",
    "    \n",
    "    for text in corpus:\n",
    "        shingles = shingling(text)\n",
    "        m = MinHash(num_perm=perms)\n",
    "        for shingle in shingles:\n",
    "            m.update(shingle.encode('utf8'))\n",
    "        minhash.append(m)\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms)  # LSH forest - более эффективная структура для поиска дубликатов, нежели обычная хеш-таблица\n",
    "    \n",
    "    for i,m in enumerate(minhash):\n",
    "        forest.add(i,m)\n",
    "        \n",
    "    forest.index()\n",
    "    \n",
    "    print('It took %s seconds to build forest.' %(time.time() - start_time))\n",
    "    \n",
    "    return forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query, corpus, forest, num_results, perms):\n",
    "    \"\"\"\n",
    "    query - string of query for duplicates searching\n",
    "    corpus - list of docs (each doc is string) - as usual\n",
    "    perms - the same\n",
    "    num_results - top results for searching\n",
    "    forest - LSH forest structure for duplicates searching\n",
    "    returns num_results docs with maximal similarity to query\n",
    "    \"\"\"    \n",
    "    query_shingles = shingling(query)\n",
    "    m = MinHash(num_perm=perms)\n",
    "    for shingle in query_shingles:\n",
    "        m.update(shingle.encode('utf8'))\n",
    "        \n",
    "    idx_array = np.array(forest.query(m, num_results))\n",
    "    if len(idx_array) == 0:\n",
    "        return [] # if your query is empty, return none\n",
    "    \n",
    "    result = corpus[idx_array]\n",
    "    \n",
    "    dedupl_res = []\n",
    "    for res in result:\n",
    "        if (res != query) and (not res in dedupl_res):\n",
    "            dedupl_res.append(res)\n",
    "                \n",
    "    print('\\n Top Recommendations: \\n')\n",
    "\n",
    "    if len(dedupl_res):\n",
    "        for i,res in enumerate(dedupl_res):\n",
    "            print (i+1, res)\n",
    "    else:\n",
    "        print(\"No similar docs =(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Протестируем, как это работает на \"Quora questions dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/quora_data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.concat([df.question1, df.question2])  # concatenate 2 columns\n",
    "texts = pd.DataFrame(texts, columns=[\"text\"])\n",
    "texts = texts[pd.notnull(texts.text)]\n",
    "corpus = texts.text.values  # get only texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построим индекс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Number of Permutations\n",
    "permutations = 128\n",
    "\n",
    "forest = get_forest(corpus, permutations)\n",
    "\n",
    "with open(\"data/quora_data/forest_dump.pkl\", 'wb') as f_obj:\n",
    "    pickle.dump(forest, f_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание структуры занимает достаточное время. Сохраним ее и загрузим из файла уже построенный индекс, чтобы сохранить время"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файлик весит много, поэтому закину его отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://t.bk.ru/aZgeg/forest_dump.pkl\n",
    "!mv forest_dump.pkl data/quora_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/quora_data/forest_dump.pkl\", 'rb') as f_obj:\n",
    "    forest = pickle.load(f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Протестируем поиск похожих, взяв в качестве запроса один из случайных документов корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = 128\n",
    "\n",
    "query = \"What are the best graduate schools for studying machine learning?\"\n",
    "result = predict(query, corpus, forest, num_results=10, perms=permutations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    rnd_idx = randint(0, len(corpus))\n",
    "    query = corpus[rnd_idx]\n",
    "    print(\"Query:\", query)\n",
    "    result = predict(query, corpus, forest, num_results=5, perms=permutations)\n",
    "    print('-------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
