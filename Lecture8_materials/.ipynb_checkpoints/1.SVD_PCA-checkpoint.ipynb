{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) TASK1.1 : PCA from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data  # we only take the first two features.\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing\n",
    "\n",
    "Перед разложением, как правило центрируют данные, чтобы признаки имели среднее=0 (иногда и дисперсию=1)\n",
    "\n",
    "Сделаем это"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Eigendecomposition - Вычисление собственных векторов и собственных значений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея PCA - разложение матрицы ковариации исхордных данных. Собственные вектора (eigenvectors) еще называются главными компонентами, поэтому метод еще называется \"Методом Главных Компонент\".\n",
    "\n",
    "Собственные вектора определяют направления в новом пространстве признаков, а собственные числа(eigenvalues) - их амплитуду. Другмими словами, собственные значения показывают дисперсию данных вдоль конкретного направления, заданного собственным вектором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление ковариационной матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix \n",
      "[[ 1.00671141 -0.11010327  0.87760486  0.82344326]\n",
      " [-0.11010327  1.00671141 -0.42333835 -0.358937  ]\n",
      " [ 0.87760486 -0.42333835  1.00671141  0.96921855]\n",
      " [ 0.82344326 -0.358937    0.96921855  1.00671141]]\n",
      "\n",
      "NumPy covariance matrix: \n",
      "[[ 1.00671141 -0.11010327  0.87760486  0.82344326]\n",
      " [-0.11010327  1.00671141 -0.42333835 -0.358937  ]\n",
      " [ 0.87760486 -0.42333835  1.00671141  0.96921855]\n",
      " [ 0.82344326 -0.358937    0.96921855  1.00671141]]\n",
      "\n",
      "Error:  -1.1449174941446927e-16\n"
     ]
    }
   ],
   "source": [
    "## Собственно посчитаем матрицу ковариации\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# так как мы уже нормализовали данные, то нет смысла вычитать из столбцов среднее значение признаков\n",
    "# но если бы мы этого не сделали, то нужно было бы сделать так:\n",
    "# mean_vec = np.mean(X_std, axis=0)\n",
    "# cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "\n",
    "cov_mat = X_std.T.dot(X_std) / (X_std.shape[0]-1)\n",
    "\n",
    "print('Covariance matrix \\n%s' %cov_mat)\n",
    "\n",
    "## и сравним с матрицей, вычисленной в numpy\n",
    "\n",
    "print('\\nNumPy covariance matrix: \\n%s' %np.cov(X_std.T))\n",
    "\n",
    "print ('\\nError: ', (cov_mat - np.cov(X_std.T)).sum()/cov_mat.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сейчас найдем собственные вектора ковариационной матрицы\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors \n",
      "[[ 0.52237162 -0.37231836 -0.72101681  0.26199559]\n",
      " [-0.26335492 -0.92555649  0.24203288 -0.12413481]\n",
      " [ 0.58125401 -0.02109478  0.14089226 -0.80115427]\n",
      " [ 0.56561105 -0.06541577  0.6338014   0.52354627]]\n",
      "\n",
      "Eigenvalues \n",
      "[2.93035378 0.92740362 0.14834223 0.02074601]\n"
     ]
    }
   ],
   "source": [
    "cov_mat = np.cov(X_std.T)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Vector Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя вычисление собственных векторов и собственных чисел ковариационной матрицы - более интуитивный способ получения главных компонент, большинство реализаций PCA под капотом выполняют SVD (более эвычислительно эффективно).\n",
    "\n",
    "Так что давайте сделаем еще и SVD, и убедимся, что результаты получатся те ми же самыми"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52237162, -0.37231836,  0.72101681,  0.26199559],\n",
       "       [ 0.26335492, -0.92555649, -0.24203288, -0.12413481],\n",
       "       [-0.58125401, -0.02109478, -0.14089226, -0.80115427],\n",
       "       [-0.56561105, -0.06541577, -0.6338014 ,  0.52354627]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v,sigma,u = np.linalg.svd(X_std.T)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Selecting Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель PCA - снижение размерности путем проекции данных на направления, задаваемые собственными векторами. Они задают только направление и их длина 1, в чем и убедимся:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "print('Everything ok!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы определиться, куда проецировать данные нужно упорядочить собственные вектора в соответствии со значением собственных чисел. Вектора с наименьшими собственными числами наименее информативны, и могут быть выброшены из рассмотрения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После сортировки нужно определиться с количеством новых признаков. Обычно используют \"explained variance\", которая показывает, какое количество дисперсии в данных описывается конкретной компонентой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая главная компонента описывает 72.77% дисперсии данных, первые две - 95.8%, первые три - 99.48%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(eig_pairs)+1), cum_var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно если из решаемой задачи нет понимания, сколько компонент нужно оставить (например, какой размер вектора сущности нужно получить, если PCA нужен для получения \"эмбеддингов\"), то следует выбирать такое количество главных компонент, которе описывает не менее 99% дисперсии данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в нашем случае хватит 3х главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но давайте попробуем сузить до 2х, чтобы отрисовать данные на плоскости, так как PCA часто используется для визуализации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), \n",
    "                      eig_pairs[1][1].reshape(4,1)))\n",
    "\n",
    "print('Matrix W:\\n', matrix_w)  # truncated matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Проекция данных  в новове признаковое пространство\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proj = X_std.dot(matrix_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как данные спроектировались на плоскость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_proj[:, 0], X_proj[:, 1], c=y, cmap=plt.cm.Set1,\n",
    "            edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - PCA in scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "а теперь воспользуемся мощью компутера и пусть scikit-learn нам сам все посчитает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "X_proj_sklearn = sklearn_pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\nError: ', (X_proj_sklearn - X_proj).sum()/X_proj.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) TASK1.2: Рекомендательные системы и SVD разложение\n",
    "\n",
    "Чтоо такое SVD? Это алгоритм, который аппроксимирует матрицу $R$ наилучшим способом путем разложения на 3 матрицы $V$, $\\Sigma$ , $U^{T}$\n",
    "\n",
    "$$\\begin{equation}\n",
    "R = V\\Sigma U^{T}\n",
    "\\end{equation}$$\n",
    "\n",
    "где \n",
    "- $R$ - матрица оценок пользователей различным фильмам (user-rating), \n",
    "\n",
    "- $V$ - матрица \"пользователь - скрытые переменные\". Эти  скрытые переменные (если интуитивно) могут отражать какие-то характеристики фильма. Например, одной из таких характеристик может быть жанр. Матрица $V$ показывает, насколько пользователю нравятся те или иные характеристики фильма, неявно выраженные через разложение. Заметим, что количество скрытых переменных - это и есть то количество компонент, которое мы хотим взять для аппроксимации исходной матрицы.\n",
    " \n",
    "- $\\Sigma$  - диагональная матрица, на главной диагонали которой стоят квадраты собственных чисел матрицы $R^TR$. Интуитивно показывает веса того, насколько важна та или иная скрытая переменная\n",
    "\n",
    "- $U^{T}$ - матрица \"фильм - скрытые переменные\". Показывает, насколько тот или иной фильм отражает ту или иную скрытую переменную. Например, вес компонента, отражающий принадлежность фильма \"Эйс Вентура: розыск домашних животных\" к скрытому признаку \"Жанр - триллер\" - будет невелик.\n",
    "\n",
    "\n",
    "Помним, что мы можем получить полное разложение и восттановить исходную матрицу обратно, но нам это неинтересно. Давайте возьмем топ $k$ комонент для разложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ratings_list = [i.strip().split(\"::\") for i in open('data/ml-1m/ratings.dat', 'r').readlines()]\n",
    "users_list = [i.strip().split(\"::\") for i in open('data/ml-1m/users.dat', 'r').readlines()]\n",
    "movies_list = [i.strip().split(\"::\") for i in open('data/ml-1m/movies_enc.dat', 'r').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.array(ratings_list)\n",
    "users = np.array(users_list)\n",
    "movies = np.array(movies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.DataFrame(ratings_list, columns = ['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype = int)\n",
    "movies_df = pd.DataFrame(movies_list, columns = ['MovieID', 'Title', 'Genres'])\n",
    "movies_df['MovieID'] = movies_df['MovieID'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взлгянем на формат данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сведем две таблицы в одну, чтобы по строкам были пользователи, по столбцам - фильмы, а на пересечении - оценка пользователя фильму.\n",
    "Если этот фильм не оценивался пользователем - заполним нулем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n",
    "R_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, перед применением SVD и PCA данные надо центрировать\n",
    "\n",
    "Давайте нормализуем матрицу per-user, вычтя для каждого пользователя среднее всех его оценок для фильма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R_df.as_matrix()\n",
    "user_ratings_mean = np.mean(R, axis = 1)  # mean per user\n",
    "R_demeaned = R - user_ratings_mean.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "\n",
    "Будем использовать ф-ю scipy `svds` для svd разложения. Выберем 50 компонент\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.sparse.linalg import svds\n",
    "V, sigma, Ut = svds(R_demeaned, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.shape, sigma.shape, Ut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma$ - вернулся как вектор квадратов собственных чисел. Давайте сконвертим его в диагональную матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.diag(sigma)\n",
    "sigma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получим аппроксимированное значение матрицы $R$ для матрицы с рекомендациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_predicted_ratings = np.dot(np.dot(V, sigma), Ut)\n",
    "all_user_predicted_ratings += user_ratings_mean.reshape(-1, 1) # вернем обратно средние оценки per-user\n",
    "all_user_predicted_ratings -= all_user_predicted_ratings.min(axis=1).reshape(-1, 1)  # пусть не будет отрицательных оценок (начинаются от 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_shifted = []\n",
    "for user_ratings in all_user_predicted_ratings:\n",
    "    if user_ratings.max() > 5:\n",
    "        user_ratings /= user_ratings.max()*5\n",
    "    \n",
    "    predicted_ratings_shifted.append(user_ratings)\n",
    "predicted_ratings_shifted = np.array(predicted_ratings_shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По-хорошему, количество компонент не выбирается вслепую. Необходимо разделить данные на train и valid, и оптимизировать RMSE(predicted, true) на валидационном наборе данных, таким образом подобрав оптимальное значение $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем порекомендовать что-нибудь кому-нибудь\n",
    "\n",
    "Давайте сделаем предсказания для конкретного пользователя. Среди тех фильмов, для которых нет оценок у данного пользователя, выведем топ фильмов с максимальным предсказанным рейтингом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(all_user_predicted_ratings, columns = R_df.columns)\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_ratings_history(userID, original_ratings_df, movies_df):\n",
    "     # Get the user's data and merge in the movie information.\n",
    "    user_data = original_ratings_df[original_ratings_df.UserID == (userID)]\n",
    "    user_full = (user_data.merge(movies_df, how = 'left', left_on = 'MovieID', right_on = 'MovieID').\n",
    "                     sort_values(['Rating'], ascending=False))\n",
    "    return user_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(userID, preds_df, movies_df, user_history, original_ratings_df, num_recommendations=10):\n",
    "    user_row_number = userID - 1 # UserID starts at 1, not 0\n",
    "    sorted_user_predictions = preds_df.iloc[user_row_number].sort_values(ascending=False)\n",
    "    sorted_user_predictions = pd.DataFrame(sorted_user_predictions).reset_index()\n",
    "    sorted_user_predictions = sorted_user_predictions.rename(columns={user_row_number : \"Score\"})\n",
    "    \n",
    "    not_rated_for_user = movies_df[~movies_df['MovieID'].isin(user_history['MovieID'])]\n",
    "    full_not_rated_predictions = not_rated_for_user.merge(sorted_user_predictions, how = 'left', left_on = 'MovieID', right_on = 'MovieID')\n",
    "    full_not_rated_predictions = full_not_rated_predictions.sort_values('Score', ascending = False)\n",
    "    top_best_rated_predictions = full_not_rated_predictions.iloc[:num_recommendations, :-1]\n",
    "    return top_best_rated_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим, каким кино интересуется пользователь с номером 837 и порекомендуем ему что-нибудь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 837"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history = get_users_ratings_history(user_id, ratings_df, movies_df)\n",
    "user_history[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_movies(user_id, preds_df, movies_df, user_history, ratings_df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неплохо!\n",
    "\n",
    "Хотя мы вообще не использовали такие признаки, как жанр фильма, модель смогла ухватить эти признаки в качестве скрытых, и предложить пользователю похожее кино по его вкусу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А давайте еще посмотрим, что у нас скрывается внутри матрицы $U$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы знаем, матрица $U$ хранит вектора для фильмов.\n",
    "\n",
    "Поищем фильмы, вектора которых являются ближайшими по евклидову расстоянию к тому фильму, айдишник которого мыф передадим в качестве запроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_movies(movie_id, Ut, R_df, top=10):\n",
    "    print(movies_df[movies_df.MovieID == movie_id])\n",
    "    \n",
    "    U = Ut.transpose()\n",
    "    vec = U[movie_id-1]  # movie_is starts from 1\n",
    "    vec_ = np.array([vec for _ in range(U.shape[0])])\n",
    "    diff = np.sqrt((U - vec_)**2)\n",
    "    closest_movie_idx = np.argsort(diff.sum(axis=1))[1:top+1]  # не берем первый элемент, потому что это тот же самый фильм\n",
    "    true_idx = R_df.columns[closest_movie_idx]\n",
    "    closest_movies = pd.DataFrame(true_idx)\n",
    "    closest_movies = closest_movies.merge(movies_df, left_on=\"MovieID\", right_on=\"MovieID\")\n",
    "    print(\"--------------\")\n",
    "\n",
    "    print(\"Closest movies\")\n",
    "    print(closest_movies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest_movies(1, Ut, R_df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже на что-то осмысленное!\n",
    "\n",
    "Здорово, что SVD вообще никак не используя информацию о жанре, научился группировать похожие фильмы. \n",
    "\n",
    "Как это стало возможно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задания\n",
    "1) Реализуйте поиск ближайших по косинусному расстоянию векторов (обычно вектора сравнивают именно так).  Отличаются ли результаты?\n",
    "\n",
    "2) Выполните поиск ближайших (по евклидовой метрике) 5 пользователей к заданному пользователю с user_id = 837. Для пользователя с каким user_id у пользователя с user_id=837 больше всего общих фильмов, оцененных на 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
